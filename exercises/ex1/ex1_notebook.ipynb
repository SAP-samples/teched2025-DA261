{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d88e7e-f3f6-4191-9950-4bbedef19861",
   "metadata": {},
   "source": [
    "# Exercise 1 - Jupyter Notebook \n",
    "## Using Isolation Forest for Outlier Analysis on financial transaction data\n",
    "SAP TechEd 2025, Hands-On Workshop: DA261 - Unlocking AI-driven insights from your business data in SAP HANA Cloud\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f789d-ec9c-417f-8ea4-a27a617e9d9a",
   "metadata": {},
   "source": [
    "### Understanding the exercise scenario "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55273c5-7199-410e-9de0-c1b6b53dfe43",
   "metadata": {},
   "source": [
    "In this exercise, you will explore how to __apply machine learning techniques__ like __Isolation Forest__ for __outlier analysis__ on __financial business transaction data__.\n",
    "- __Financial business transaction data__ of most SAP applications is managed in the central tables like [Universal Journal](https://help.sap.com/docs/SAP_S4HANA_ON-PREMISE/651d8af3ea974ad1a4d74449122c620e/523b8a55559ad007e10000000a44538d.html?locale=en-US&version=LATEST), ACDOCA and accessible through a large variety of CDS views like for example [I_GLAccountLineItemRawData](https://help.sap.com/docs/SAP_S4HANA_CLOUD/c0c54048d35849128be8e872df5bea6d/7fe239a3f2214e2cb36e90d453eee6d3.html) for different purposes. The Universal Journal, ACDOCA table, is one of the tables in S/4HANA systems with the largest number of links to business transactions, and thus also is typically a very large table within the system.\n",
    "    - As reference information and background reading, see the following blog posts [Analytics on Universal Journal, the heart of SAP S/4HANA](https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-sap/analytics-on-universal-journal-the-heart-of-sap-s-4hana/ba-p/13489661) and [Understanding the Universal Journal in SAP S/4HANA](https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-sap/understanding-the-universal-journal-in-sap-s-4hana/ba-p/13345726).  \n",
    "- Given the variety in nature of business transaction managed in the Universal Journal, __outlier analysis__ as a form of __analytics on the universal journal__ involves a as well a great variety of techniques in order to detect errors or fraudulent transactions. \n",
    "    - Examples would be detecting outlier by applying certain rules in searching through the data for false or incomplete booking, e.g. missing transaction types, missing functional areas, wrong accounts for business scenario, balance not zero in certain account, and so on and on.\n",
    "\n",
    "<br>\n",
    "\n",
    "In this exercise, we introduce a __trending machine learning__ technique for __outlier analyis__ called __isolation forest__, \n",
    "- the technique uses a ensemble set of multiple decision trees and identifies outliers based on the assumption that the __decision tree depths for outliers is shorter__ than the average decision tree depths of normal values;\n",
    "- the technique can also be applied to financial business transaction data, and can be applied within the SAP HANA database,\n",
    "- thus __adding to the mix of outlier detection methods__, it can be __helpful__ to __identify outliers in the business transaction data by their unusual pattern of data values__. \n",
    "- Furthermore, the technique is also well known as __capable of analyzing larger amounts of data__.\n",
    "\n",
    "Data used for the analysis\n",
    "- Outlier analysis on financial business transaction data typically __focuses on specific, smaller subsets or slices of the data__, e.g. business area with specific and typical financial transaction data pattern. Therefore outlier analysis will never be applied to the complete table at once.\n",
    "- Hence, use of filtering for subsets and slices and specific CDS views is common practice\n",
    "- Potential extraction of data for further analysis to a side by side SAP HANA Cloud system is possible via the Smart Data Access (SDA) ABAP adapter used to consume remote ABAP CDS Views, see this blog post for details [Taking Data Federation to the Next Level: Accessing Remote ABAP CDS View Entities in SAP HANA Cloud](https://community.sap.com/t5/technology-blog-posts-by-sap/taking-data-federation-to-the-next-level-accessing-remote-abap-cds-view/ba-p/13635034).\n",
    "- In this exercise now, we use an artificially generated small table called ACDOCA. For details on how the data is generated, see the code in the appendix of the notebook.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Overview on the exercise tasks\n",
    "\n",
    "![](./images/ex1_scenario.png)\n",
    "\n",
    "\n",
    "<br>\n",
    "Not let's get started with the exercise!\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b07eb-11ef-41d6-a9db-2c0ffe1e536f",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Ex. 1.0 - Connect to your SAP HANA Cloud instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b631401-730a-43c1-bfd9-a2223d771b7c",
   "metadata": {},
   "source": [
    "### Step 0: Establish and check connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f411a-9d0f-4987-a1d8-b65e686b100f",
   "metadata": {},
   "source": [
    "Throughout this exercise, we will be using the __Python Machine Learning client for SAP HANA (hana-ml)__, as your reference to all its functions and capabilities see the [hana-ml documentation](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/change_log.html). The current version released with SAP HANA Cloud 2025 Q3 release is 2.26.\n",
    "- The python package hana-ml in general allows to script in python, while SQL code is generated on-the-fly and directly passed to a connected SAP HANA database system for execution.\n",
    "    - It allows to access and prepare data by means of a HANA dataframe, a python object holding a SQL select query. Many methods are provided to be used with the HANA dataframe, changing the SQL select query behind the scenes.\n",
    "    - As its core, it provides methods to apply AI functions (algorithms from the [Predictive Analysis Library (PAL)](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/sap-hana-cloud-sap-hana-database-predictive-analysis-library-pal?locale=en-US&version=LATEST) and [Automated Predictive Library (APL)](https://help.sap.com/docs/apl?locale=en-US&version=LATEST) to the data prepared with a HANA dataframe, designed to apply all the processing within the SAP HANA database.\n",
    "- The python package is released as a component with every the SAP HANA Client delivery, in addition the latest __hana-ml__-version can always be found at the pypi public repository at https://pypi.org/project/hana-ml/.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf59397-e4f1-48e9-823e-2b1b199ea266",
   "metadata": {},
   "source": [
    "In Python, installed packages require to be __imported__ to the current session, in order to be available for use in python scripts.\n",
    "- Let's first run the import of hana-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98fc5e-2925-45ea-8bcc-7adc83cb5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Python Machine Learning client library for SAP HANA and get the version\n",
    "import hana_ml\n",
    "print(hana_ml.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8af1d-8b89-483c-931e-51e25944d734",
   "metadata": {},
   "source": [
    "Next, execute the next cell and the referenced script, prepared in the Getting Started section to connect to the SAP HANA CLoud database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34658c-c04d-4223-8b4b-1dae43d889bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../ex0/ex0_2-check_setup.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9a97a-deda-47b6-ba6f-52dbddb4ddb4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Ex. 1.1 - Exploring financial transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef596ef-5f94-4a8c-a52e-d8b89456084b",
   "metadata": {},
   "source": [
    "### Step 1: Create HANA dataframe for the financial business transaction sample table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8b8f6-90bb-4ff5-85b7-d54dfd4f2ec5",
   "metadata": {},
   "source": [
    "__Introduction to SAP HANA dataframes__\n",
    "- The [HANA dataframe](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/hana_ml.dataframe.html#module-hana_ml.dataframe) represents a database query as a hana-ml dataframe object in python, comarable to a pandas dataframe. \n",
    "  - Most HANA dataframe operations are designed to NOT bring data back from the database into the python envrionment, unless it is a small aggregated result set or explicitly requested. \n",
    "- SAP HANA dataframes can be created\n",
    "    - based on database tables, SQL views and calculation views (incl. parameters), custom SQL statements incl. multi-statements\n",
    "    - or created from pandas dataframe or spark dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e53774-f3a9-452b-a7fe-b0a8a8f25c43",
   "metadata": {},
   "source": [
    "Based on SAP HANA Cloud database instance [ConnectionContext-object](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/hana_ml.dataframe.html#hana_ml.dataframe.ConnectionContext) __\"myconn\"__, we are using the __table-method__ to create the initial HANA dataframe.\n",
    "- Note, ConnectionContext is a child-object to the dataframe-class, that's why we can apply the table- and other methods to create a HANA dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ee617-e66b-4a84-bd97-3b37522ea455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the connection using \"myconn\"\n",
    "myconn.connection.isconnected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11644728",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Creating a HANA dataframe __acdoca_hdf__ against the database table ACDOCA demo table, containing artificially generated data and columns. See the appendix for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf1dff-f9f9-46ab-9eb3-c71213cc3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a HANA dataframe in Python against the HANA Cloud table\n",
    "acdoca_hdf = myconn.table(\"ACDOCA\", schema=\"DA261_SHARE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da650c22-9426-4245-8c91-25913d675c52",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Explore what a HANA dataframe object is within the python environment. \n",
    "- The attribute __select_statement__ shows the current HANA dataframe SQL select of the dataframe\n",
    "- The python methods __print()__ or __display()__ present the output of executed python commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b39df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand what is the HANA dataframe\n",
    "print(acdoca_hdf)\n",
    "print(acdoca_hdf.select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c752ce7-cbb9-422d-adc3-5858e6c187b6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Understand the data structure of the query set underlying the dataframe\n",
    "- using the shape, columns and dtypes() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a9b26-8f8f-4eea-8199-8c02c3ebce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the data structure of the query set underlying the dataframe\n",
    "print(acdoca_hdf.shape, '\\n')\n",
    "print(acdoca_hdf.columns, '\\n') \n",
    "display(acdoca_hdf.dtypes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc0721-0f0f-4066-8804-3a74853033fc",
   "metadata": {},
   "source": [
    "As the shape method output indicates, the table we use for the purpose of this session is very small and has ony 500 rows and 13 columns.  \n",
    "The data will be explored in more detail by the following dataframe methods and theirs output.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9315ba-ed5a-4fa5-945a-6561ed13f8f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For an __initial view at the data__, the __head()__- in conjunction with the __collect()-method__ can be used.\n",
    "\n",
    "- __Only__ when the __collect()-method__ is used, a __HANA dataframe's SQL query result set data__ is actually __transferred into the python envrionment__. \n",
    "- Therefore, __carefully__ make use of __collect()-method__, best __Do not__ use collect() without any further filtering or aggregation methods applied.\n",
    "- The head()-method for example, adds the TOP N predicate to the SQL select statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2eef2d-cede-4771-a25f-b1fcf6a042c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on the TOP 5 rows, and show them in python\n",
    "display(acdoca_hdf.head(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e4476-f3e6-4a84-9651-1d51c74c687d",
   "metadata": {},
   "source": [
    "<br>\n",
    "The dataframe and its method applied, is still only a SQL query statement. So how does it look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263096d7-0155-49a4-9b94-01fa6a9edd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acdoca_hdf.head(5).select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adeaa32-b74f-4313-ae73-4325ba3965ef",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Step 2: Explore the ACDOCA data using HANA dataframe methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950afead-579a-44d0-ad25-8a503f9c6884",
   "metadata": {},
   "source": [
    "Let's look as some exemplary filtering and aggregation methods for the HANA dataframe\n",
    "- Using a column list with the select()- and the head(5)-method to filter the result set rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb98f9-8ea1-44d7-a7e0-3662824d22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns using the select-method and list of columns\n",
    "acdoca_hdf.select('Company Code', 'G/L Account', 'Profit Center', 'Financial Account Type','Amount (Transaction)').head(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4009d-9149-4c73-80bd-604d5e6f1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, it's just a change to the select statement query of the dataframe\n",
    "acdoca_hdf.select('Company Code', 'G/L Account', 'Profit Center', 'Financial Account Type','Amount (Transaction)').head(5).select_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f422e7a-998f-4c1b-9ab7-d437c950e7dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Filtering data rows, by applying SQL WHERE-clause expressions using the filter-method\n",
    "- Escaping of 'string'-values in the expression using \\\\'string\\\\' is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dbfcb-854c-4746-87e0-2296974a2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by rows, the filter-method applies a SQL where.clause  [Note, string-quotes \"'\" within the where-expression need to be escaped using \"\\\"]\n",
    "acdoca_hdf.filter('\"Company Code\" = \\'CC01\\' AND \"Profit Center\"=\\'PC002\\'').head(5).collect()\n",
    "\n",
    "# As to your interest, explore the select statement as well uncommenting the next line\n",
    "# acdoca_hdf.filter('\"Company Code\" = \\'CC01\\' AND \"Profit Center\"=\\'PC002\\'').select_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aacb79e-f61f-45ae-8969-fe76190707dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "Combining it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac040f-1df9-44de-b995-bc4cae000bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(acdoca_hdf.select('G/L Account', 'Profit Center', 'Financial Account Type','Amount (Transaction)').filter('\"Profit Center\"=\\'PC002\\'').head(5).collect())\n",
    "print(acdoca_hdf.select('G/L Account', 'Profit Center', 'Financial Account Type','Amount (Transaction)').filter('\"Profit Center\"=\\'PC002\\'').head(5).select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4929bcb-f2ac-4fe1-91df-451a7037330b",
   "metadata": {},
   "source": [
    "As you can determine by the __nested SQL statement__, the __order of method-calls__ matters (here select>filter>head) and impacts the results.  \n",
    "For data inspection using a dataframe, for safeguarding it is a good practice to always consider using the head()-function before the collect()-call.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a139801-18d2-4132-9403-0d362dcb746f",
   "metadata": {},
   "source": [
    "Creating a new HANA dataframe from an existing dataframe\n",
    "- New dataframe = existing dataframe.\\<methods ...\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7663d-1235-4dcc-943b-d5b88b0c79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, we are not using the collect()-method, as we don't want to transferring any data\n",
    "hdf_acdoca_tmp=acdoca_hdf.select('G/L Account', 'Profit Center', 'Financial Account Type','Amount (Transaction)').filter('\"Profit Center\"=\\'PC002\\'').head(5)\n",
    "print(hdf_acdoca_tmp.select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f58ab-6bd0-45b0-9967-676c1c335e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "## Ex 1.2 - Outlier analysis using IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba18bb0-d99e-4094-94f6-541f1eeb5dd0",
   "metadata": {},
   "source": [
    "### Step 3: Execute basic outlier analysis using Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e565c3-7f4a-4bab-9640-89c7d254a5ee",
   "metadata": {},
   "source": [
    "Now, let's explore our financial business transaction data for outliers, \n",
    "- filtering the HANA dataframe for Company Code CC01 data and for Profit Center PC002,  \n",
    "- and prepare a HANA dataframe __hdf_acdoca_slice__ as the input data set to our Isolation Forest analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19427ec1-7145-4218-9c8c-200d92af5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_acdoca_slice=acdoca_hdf.filter('\"Company Code\" = \\'CC01\\' AND \"Profit Center\"=\\'PC002\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeca929-7996-49c6-b4bf-bf3137ec6ec5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The __Isolation Forest__-function in SAP HANA Cloud [PAL SQL reference](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/isolation-forest-isolation-forest-11345d9?q=evaluation_metric&locale=en-US&version=LATEST), [hana-ml Python reference](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/pal/algorithms/hana_ml.algorithms.pal.preprocessing.IsolationForest.html#isolationforest) builds an __esemble model__ of __multiple decision trees__ (thus called __\"forest\"__). \n",
    "- It tries to detect or isolate outliers based on the assumption that the __decision tree depths__ for __outlier values__ is __shorter__ than the average decision tree depths for normal values.  \n",
    "- It is regarded as a technique also suitable to be applied to large datasets.  \n",
    "- For additional background reading on algorithm, see also [wikipedia: isolation forest](https://en.wikipedia.org/wiki/Isolation_forest) and [wikipedia: different outlier detection techniques](https://en.wikipedia.org/wiki/Anomaly_detection) for additional background reading.\n",
    "![](./images/ex1_IF_algorithm.png)\n",
    "\n",
    "Amongst other capabilities, the __SAP HANA Cloud Isolation Forest-functions__ supports for\n",
    "- analysis of outliers within a __mixed feature set__ of numeric as well as __categorial__ columns,\n",
    "- provides __AI explainability__ insights for predicted outliers\n",
    "- allows for __massive data-parallel outlier analysis__ on __multiple subsets_ of data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a7cae-84fa-4745-a1e4-deede05d5a69",
   "metadata": {},
   "source": [
    "<br>\n",
    "Preparing the feature set as a list of columns we seek to consider for the outlier detection, incl. some categorial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca636ee-0d0e-4750-9a2d-5c1fc57ccbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_features=['Debit/Credit', 'Accounting Document Type', 'Transaction Type', 'Financial Account Type', 'Amount (USD)', 'Amount (Transaction)'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3d51d-efce-4f7c-8f11-d2ffee52d66c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now in the first step, we will use the __fit-method__ to __build the Isolation Forest model__, and __apply__ the model with the __predict-method__ in second step to retrieve the detected outliers.\n",
    "- If we seek to detect outliers in very large datasets, the outlier detection Isolation Forest model could be trained on a representative and larger enough sample of the data,\n",
    "- while the predict method with the trained model can then applied to the full data set. \n",
    "- The predict-task is also a row-independent task and thus various parallel invocation techniques can be applied (e.g. parallel by partition, by value, ...) for a faster performance.  \n",
    "\n",
    "We are applying the Isolation Forest method with the __default algorithm parameter values__, which are\n",
    "- __n_estimators=100__, which specifies the number of trees the model will be composed of.\n",
    "- __max_samples=256__, which refers to the number of sample rows to draw from the input data to train each tree. \n",
    "- __bootstrap=False__, row sampling happens without replacement, thus each tree is build from a different set of sample rows.\n",
    "- __random_state=251104__, can be set to any value. It simply determines repeatability by setting a fix starting point for randomness within the algorihm.\n",
    "\n",
    "For details on how to set Isolation Forest parameter values \n",
    "- see the reference section in the Appendix of the notebook __Setting Isolation Forest parameter values with larger datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3d84b-37b6-454f-8a14-c9bfe7faa04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Isolation Forest method class\n",
    "from hana_ml.algorithms.pal.preprocessing import IsolationForest\n",
    "\n",
    "# Creating our IsolationForest model object names \"isof\"\n",
    "isof = IsolationForest(random_state=251104, n_estimators=100, max_samples=256, bootstrap=False)\n",
    "\n",
    "# Executing the fitting, i.e. the training of the Isolation Forest Outlier model\n",
    "isof.fit(data=hdf_acdoca_slice, features=outlier_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e5964-f483-4b7c-abc8-fbad0bd60253",
   "metadata": {},
   "source": [
    "As the fit method in this case doesn't present results, let's inspect what has been executed on the SAP HANA Cloud database.\n",
    "- There a list a [base-methods](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/pal/algorithms/hana_ml.algorithms.pal.pal_base.PALBase.html#hana_ml.algorithms.pal.pal_base.PALBase) available for all PAL algorithms in HANA-ML, helpful to explore generated or executed sql. As an example we will use __\"get_fit_execute_statement\"__\n",
    "- In addition, there is the __last_execute_statement__-method of the connection object, which sometimes is also helpful to quickly inspect what has been executed at last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887fe41-4c2f-4380-a235-8c822895eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What has just been executed in the SAP HANA CLoud datbase?\n",
    "print(isof.get_fit_execute_statement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f827e75-445c-473d-bd7b-67a24ffd1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, what has been last executed over the connection?\n",
    "#print(myconn.last_execute_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941c17e-55d7-4402-9684-2028e838ccd4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, in this second step, we want to __predict the outliers__ in our data using the isolation forest model we just created\n",
    "- While in this exercise step, we are using the same data set, this could as well be a different, much larger one or an updated data set. Provided it has the same structure and columns.\n",
    "- Alike with other AI functions and algorithms, the predict-method requires and a key-/ID-column to be able to match the predicted results with the input data used for to generate the predictions.\n",
    "\n",
    "First we therefore create a simple ID-column using the __add_id()-method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0173b5d-f5a3-4f59-9888-9741c44e01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of this demo, we're a using a simple add_id()-method. Within your real data, likely one would create a key-column over a set of composite logical key columns.\n",
    "hdf_acdoca_slice_id=hdf_acdoca_slice.add_id()\n",
    "\n",
    "hdf_acdoca_slice_id.head(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335bb37-cd30-4079-bfaa-ef5d08641d1f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, let's execute the __predict-method__\n",
    "- The __most important parameter__ we set is the __contamination value__, it's basically the __proportion of expected outliers__ in the data set. \n",
    "- Thus the value should be set by the data analyst, familiar with the business context of the dat.\n",
    "- Here, let's start with 0.05, i.e. expecting 5% of outliers in our data\n",
    "- Try it for different values like 0.01, 0.02 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6f6fe-b6a3-4bfd-8168-a7f22aa73b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_results = isof.predict(data=hdf_acdoca_slice_id, key='ID', features=outlier_features, \n",
    "                               contamination=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb13a16",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The result object of the __isof.predict()__ call is again HANA dataframe, let's take a closer look .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039939e-4596-4279-8551-820022e84f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are persisted into a temporary database table, identified by the HANA dataframe \"outlier results\".\n",
    "outlier_results.select_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c631f0-f587-4a7f-9495-17664942dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to explore, what has been executed within the SAP HANA database\n",
    "# print(isof.get_predict_execute_statement())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee8666-47cc-4005-87fb-b3c1efba0ac4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now let's explore the __Isolation Forest outlier analysis results__\n",
    "- The result dataframe presents the columns ID, __SCORE__ and __LABEL__\n",
    "- __LABEL__ values of __-1__ indicate, the __data row has been identified as an outlier__\n",
    "- The __SCORE (or anomaly score)__ value __quantifies how easily__ a data point could be isolated from the rest of the data, and thus classified as an outlier or not.\n",
    "    - It is calculated based on the average path length in the trees of the isolation forest, with __shorter path lengths__ indicating __higher anomaly scores__ (closer to 1),  \n",
    "  suggesting the point is an anomaly, while longer path lengths indicate lower anomaly scores (closer to 0) and are considered normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae85c6b-1d4d-4309-b334-dd717b97dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort outlier results descending by SCORE-value\n",
    "display(outlier_results.sort('SCORE', desc=True).head(5).collect())\n",
    "\n",
    "# Count number of predicted outliers (LABEL = -1) and non-outliers (LABEL = 1)\n",
    "display(outlier_results.agg([('count','ID','n_transactions')] ,group_by = ['LABEL']).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563be11-02fe-4b31-8863-11e74c6b067c",
   "metadata": {},
   "source": [
    "The first results set presents the top 5 predictions, \n",
    "- using the sort-method with the result dataframe, sorting the results descendingly based on the SCORE-values.\n",
    "The second result set aggregates the prediction results and \n",
    "- counts the predication based on the LABEL values. Thus we can conclude we could identify 9 outlier records in our data.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff4240-3713-4668-b31a-12c378e76ca5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, let's visualize the outlier scores and how they distribute across the amount values, using a Plotly visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40589503-fd4d-4cd6-8b43-5f75bbd02607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the outlier scores and how they distribute across the amount values, using a Plotly visualization\n",
    "# Export the results into a pandas dataframe for visualization in Python, in this case it is still a small set of data points\n",
    "pdf=hdf_acdoca_slice_id.select('ID', ('\"Amount (Transaction)\"', 'AMOUNT')).set_index(\"ID\").join(outlier_results.set_index(\"ID\")).sort('SCORE', desc=True).collect()\n",
    "pdf['LABEL'] = pdf['LABEL'].astype(str)\n",
    "\n",
    "# Building a custom scatter plot\n",
    "import plotly.express as px\n",
    "fig = px.scatter(pdf, x=\"AMOUNT\", y=\"SCORE\", color_discrete_map = {\"1\": \"blue\", \"-1\":\"red\"}, color=\"LABEL\", width=800, height=400)\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913633d-01a0-405c-82c3-d6d9f7fc3351",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "One first approach in trying to understand the outlier classification predictions by the model, is to join the results back with the input data.\n",
    "- here we use the HANA dataframe __join-method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6128a0-d7c7-48c5-82e6-0046463a6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dataframes with each other, applying set_index as the inner join key column; restricting to the columns we have used as features with Isolation Forest\n",
    "hdf_acdoca_slice_id.select('ID', 'Debit/Credit', 'Accounting Document Type', 'Transaction Type', 'Financial Account Type', 'Amount (USD)', 'Amount (Transaction)'\n",
    "                                          ).set_index(\"ID\").join(outlier_results.set_index(\"ID\")).filter('LABEL = -1').sort('SCORE', desc=True).head(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27137fe8-c730-44ca-9d47-cf29354eea0f",
   "metadata": {},
   "source": [
    "Depending on our familiarity and expertise with the data, it might still be difficult to understand the outlier decisioning done  by the Isolation Forest model.  \n",
    "Therefore, let's try to get more insights applying AI explainability methods.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d853a-1c5a-43bc-9e4d-9e3bbd823ccc",
   "metadata": {},
   "source": [
    "### Step 4: Add shapley explanations to Isolation Forest outlier predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d6c41-498c-4a5c-8553-c33fbbb8e2d5",
   "metadata": {},
   "source": [
    "The SAP HANA Predictive Analysis Library provides and extensive set of __AI explainability__  approaches for [classification](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/local-interpretability-of-models-local-interpretability-of-models-c330665?locale=en-US&version=LATEST), regression as well as time series functions predictions.\n",
    "In AI explainability we try to get transparency at level of the AI / machine learning model, as well as the individual predictions, therefore we distinguish between\n",
    "- __Global AI explainability__ as analysis and reports, presenting the overall model insights, \"global\" overall contribution of individual features to the model. See this [blog post](https://community.sap.com/t5/technology-blog-posts-by-sap/global-explanation-capabilities-in-sap-hana-machine-learning/ba-p/13620594) if interested in more details;\n",
    "- __Local AI explainbility__, as explaitions for individual predictions, which feature values contributed to which degree to the models prediction outcome. \n",
    "\n",
    "As a very commonly applied AI explainability method, the so-called __Shapley Additive Explanations(SHAP)__ have become a widely used standard\n",
    "- Shapley Additive Explanations(SHAP) provide explainabilty models derived from game theory, indicating which feature-values for a given prediction had the largest impacted in a decision.\n",
    "- For more details see this community blog post [On Responsible AI: SHAP of you](https://community.sap.com/t5/technology-blog-posts-by-members/on-responsible-ai-shap-of-you/ba-p/13553641)\n",
    "- The hana-ml package provides a series of methods and visualization, see the documentation for [local_interpretability](https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.07/en-US/pal/topics/local_interpretability.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a26f38-10e1-4cd6-9441-613c263b819a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The __SHappley Additive Explainations__ can also be utilized with the SAP HANA Cloud __Isolation Forest__-function (since 2025 Q2).\n",
    "- Local AI explainability is activated in the predict function using __show_explainer=True__-parameter, where the explanation output is generated into the REASON_CODE column\n",
    "- Due to its processing impact, it is recommended not to enable it by default and use with __explain_scope__ to explain only the outliers.\n",
    "- With __top_k_attributions__ we can limit the explanation outout within the REASON_CODE column to the __top k columns within the individual explanations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a43072-3197-481f-9d3b-c6a41e37c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_results_explained = isof.predict(data=hdf_acdoca_slice_id, key='ID', features=outlier_features,\n",
    "                       contamination=0.05,\n",
    "                       show_explainer=True, explain_scope='outliers', top_k_attributions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad939d65-7a56-4451-9213-961ef9d111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the RESON_CODE explanations of the predicted outliers in full width\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None) \n",
    "outlier_results_explained.filter('LABEL = -1').head(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e276a0-476c-4764-99d0-09a6b134289c",
   "metadata": {},
   "source": [
    "How to read and interpret the results in the REASON_CODE column, \n",
    "- for ID 17, the top 1 column in the explanation result is: {__\"attr\":\"Transaction Type\"__,\"val\":0.4292705364764549,__\"pct\":26.700000000000004__}\n",
    "- __\"Transaction Type\" is the top 1 attribute, contribution to 26.7% to the outlier prediction__ (classification as outlier and SCORE value). \n",
    "- \"val\" is the SHAPley value, which on its own cannot be explained here.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "516f484f-00a7-4802-a830-f06c8ab23743",
   "metadata": {},
   "source": [
    "# If you seek to decompose the REASON_CODE column into individual columns use this example [cell is set a type RAW, change to type Python befor execution]\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "outlier_results_explained.select('ID', 'SCORE', 'LABEL'\n",
    "                ,('json_query(\"REASON_CODE\", \\'$[0].attr\\')', 'Top1'), ('json_query(\"REASON_CODE\", \\'$[0].pct\\')', '[%Top1]') \n",
    "                ,('json_query(\"REASON_CODE\", \\'$[1].attr\\')', 'Top2'), ('json_query(\"REASON_CODE\", \\'$[1].pct\\')', '[%Top2]') \n",
    "                ,('json_query(\"REASON_CODE\", \\'$[2].attr\\')', 'Top3'), ('json_query(\"REASON_CODE\", \\'$[2].pct\\')', '[%Top3]') \n",
    "                , 'REASON_CODE' \n",
    "          ).filter(\"LABEL < 0\").head(3).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740d454-b69d-486c-aeed-905884c2abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now joining the results again, with the input features for better reasoning of the explanaition results\n",
    "hdf_acdoca_slice_id.select('ID', 'Debit/Credit', 'Accounting Document Type', 'Transaction Type', 'Financial Account Type', 'Amount (USD)', 'Amount (Transaction)'\n",
    "                                          ).set_index(\"ID\").join(outlier_results_explained.set_index(\"ID\")).sort('SCORE', desc=True).head(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe183f-41a6-4b35-bc06-23f7d046a0b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can review __global AI explainabiltiy__, overall insights on the level fo the isolation forest-model using the __Shapley Explainer summary plot__\n",
    "- Select the __Feature Effects__ on the __Bar-Plot tab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1361b8-020e-49a6-b2e8-ea036db1751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global AI Explainability results, inspect the Feature Effects on the Bar Plot of the visualization\n",
    "from hana_ml.visualizers.shap import ShapleyExplainer\n",
    "shapley_explainer = ShapleyExplainer(reason_code_data=outlier_results_explained.sort('ID').select('REASON_CODE'), feature_data=hdf_acdoca_slice_id.sort('ID').select(outlier_features)) \n",
    "\n",
    "# Pipeline model, glocal shapley model explanations\n",
    "shapley_explainer.summary_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4903cd-a81f-4e7e-92c4-369f375fd9c9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The ShapleyExplainer-object, also provides another __local AI explainability__ visualization, the the so-called __Shapley Force plot__.  \n",
    "- As an example to understand the Force Plot, select row 56 in the plot, it shall match ID 57 in the data\n",
    "- Expand the force plot visualization, clicking the \"+\" icon to the left of the row\n",
    "- The __shapley values__, not the percentage values are __displayed with their positive / negative values__ and impact to the classification decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647d0a0-c3f3-4ad7-9700-78d433dd86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Force plot, for a visualization of the local AI explainability, Shapley values\n",
    "shapley_explainer.force_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7e0c2-aa59-4df0-b164-7fdabd287005",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Ex 1.3 - Outlier analysis per subgroup using Massive Isolation Forest (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affaeef2-6150-4e33-b93b-6b5cc7d6994c",
   "metadata": {},
   "source": [
    "The __Isolation Forest__-funtion of the __Predictive Analysis Library__ in SAP HANA CLoud recently (release 2025 Q2) introcuded [massive, data-parallel outlier analysis](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/isolation-forest-isolation-forest-11345d9#ariaid-title6). \n",
    "- This allows to run isolation forest analysis tasks for each subgroup independently and in parallel for a maximum of performance and outcome.\n",
    "- This __massive, data-parallel__ pattern is a commonly used approach within SAP applications applying for example times series forecasting, at the scale of modelling and forecasting multiple thousand time series in parallel. \n",
    "- __Resource consumption__ can be fully controlled by SAP HANA's __Workload Management__ capabilities, i.e. by __limiting the number of available threads__ for the task, which slows down the overall task by reducing the degree of parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34956496-6974-4112-a848-0ff50a2acae2",
   "metadata": {},
   "source": [
    "### Step 5: Data-parallel outlier analysis per \"G/L Account\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10161ce-422c-4224-b21f-d7500486229f",
   "metadata": {},
   "source": [
    "Let's apply the massive, data-parallel isolation forest to the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775c23a-3e84-4306-a479-94069a7e648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review our data and features\n",
    "print(outlier_features, '\\n')\n",
    "display(hdf_acdoca_slice.head(2).collect())\n",
    "display(hdf_acdoca_slice_id.head(2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fcb92-e771-4bfc-a4c0-716c351233c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's search for outliers in our data slice, for each \"G/L Account\" in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af49701-5541-478d-b4aa-d4b5fbf34cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many \"G/L Account\" values do we have in our data?\n",
    "hdf_acdoca_slice.distinct(\"G/L Account\").agg([('count','G/L Account','Count of G/L Accounts')]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9b091",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The IsolationForest object parameter __massive=True__ and the fit-/predict-method parameter __group_key=\"G/L Account\"__ trigger the massive data-parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411da60d-dfa3-49dd-96e0-67582cd4dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.pal.preprocessing import IsolationForest\n",
    "parallel_isof = IsolationForest(massive=True, random_state=2, n_estimators=100, max_samples=256, bootstrap=False)\n",
    "\n",
    "parallel_isof.fit(data=hdf_acdoca_slice, group_key=\"G/L Account\", features=outlier_features)\n",
    "\n",
    "res, err = parallel_isof.predict(data=hdf_acdoca_slice_id, key=\"ID\", group_key=\"G/L Account\", features=outlier_features                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a9a0e-4aff-4c55-9313-0f484e302324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(parallel_isof.get_fit_execute_statement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a87bde-e0c8-440c-893e-d13af93f334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results show outlier predictions across the data groups of \"G/L Accounts\"\n",
    "display(res.sort('SCORE', desc=True).head(10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301f054-cbdb-4fa8-a885-5d8d20650cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join original value with outlier data\n",
    "hdf_acdoca_slice_id.select('ID', 'G/L Account', 'Amount (USD)', 'Amount (Transaction)').set_index(\"ID\").join(res.set_index(\"ID\")).sort('SCORE', desc=True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ccd3d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've now completed exercise 1, great !\n",
    "\n",
    "Continue to - [Exercise 2 - Analyzing consumer complaints using text embeddings and machine learning](../ex2/README.md)\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Further reference information and examples \n",
    "Note the appendix section of the ex1_notebook.ipynb-file might include additional expert-level details for your offline study, incl. reference to the data used in the exercise.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c353f72-7d14-48f5-b343-5257e6cd39fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix - reference sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cde08",
   "metadata": {},
   "source": [
    "## Code generation for design-time applications (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd19bd5",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "You are happy with the outlier analysis scenario isolation forest model and now seek to embedd it into your design-time application.\n",
    "1. utilize methods from the [hana-ml.PAL.PAL-base subclass](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_3_QRC/en-US/pal/algorithms/hana_ml.algorithms.pal.pal_base.PALBase.html) which are implicitly available with all PAL algorithms objects in hana-ml\n",
    "<br> \n",
    "    ![](./images/ex1-appd-dt-basemethods.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. utilize methods from the [hana-ml.artifacts package](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_3_QRC/en-US/hana_ml.artifacts.html) to generated ABAP AMDP, CAP or HANA native HDI (HANA deployment infrastructe) design-time artifacts for a respective project\n",
    "    - SQL tracing requires to be enabled\n",
    "    - At least both fit- and predict-method have to be executed for generating persistant artifacts from traced execution code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e93fa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Approach 1.) __Review recently generated code and temporary objects__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ae84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_features=['Debit/Credit', 'Accounting Document Type', 'Transaction Type', 'Financial Account Type', 'Amount (USD)', 'Amount (Transaction)'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bda1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Isolation Forest method class\n",
    "from hana_ml.algorithms.pal.preprocessing import IsolationForest\n",
    "\n",
    "# Creating our IsolationForest model object names \"isof\"\n",
    "isof = IsolationForest(random_state=251104)\n",
    "\n",
    "# Executing the fitting, i.e. the training of the Isolation Forest Outlier model\n",
    "isof.fit(data=hdf_acdoca_slice, features=outlier_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df605d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isof.consume_fit_hdbprocedure(\"ISOF_OUTLIER_BASE_PROC_NAME\")['base'], \"\\n\")\n",
    "#print(isof.fit_hdbprocedure)\n",
    "print(isof.get_fit_output_table_names())\n",
    "print(isof.get_fit_parameters())\n",
    "\n",
    "print(isof.consume_fit_hdbprocedure('<PROC_NAME>', in_tables=[\"<ANALYSIS_DATA>\"], out_tables=[\"<IF_MODEL_TABLE>\"])['consume'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d09df",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Approach 2.) __Now, generate the full design-time artifacts along with other dev-project required files like synonyms etc.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a73125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of later artifact generation, enable sql tracing\n",
    "myconn.sql_tracer.enable_sql_trace(True)\n",
    "myconn.sql_tracer.enable_trace_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00377c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Isolation Forest method class\n",
    "from hana_ml.algorithms.pal.preprocessing import IsolationForest\n",
    "\n",
    "# Creating our IsolationForest model object names \"isof\"\n",
    "isof_dev = IsolationForest(random_state=251104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the fitting, i.e. the training of the Isolation Forest Outlier model\n",
    "isof_dev.fit(data=hdf_acdoca_slice, features=outlier_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79623bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_results = isof.predict(data=hdf_acdoca_slice_id, key='ID', features=outlier_features, \n",
    "                               contamination=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7792bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What has been captured as objects in the SQL tracer log?\n",
    "print(myconn.sql_tracer.trace_sql_log.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the trace object at the next level of the fit object?\n",
    "print(myconn.sql_tracer.trace_sql_log['IsolationForest1'].keys())\n",
    "print(myconn.sql_tracer.trace_sql_log['IsolationForest1']['fit'].keys())\n",
    "\n",
    "#print(myconn.sql_tracer.trace_sql_log['IsolationForest1']['predict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And what does the SQL object look like for the fit call?\n",
    "display(myconn.sql_tracer.trace_sql_log['IsolationForest1']['fit']['sql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(myconn.sql_tracer.trace_sql_log['IsolationForest1']['fit']['output_tables'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.artifacts.generators import hana\n",
    "from hana_ml.artifacts.generators.hana import HANAGeneratorForCAP\n",
    "hanagen = HANAGeneratorForCAP(project_name=\"OutlierAnalysis_HANA-CAP\",\n",
    "                              output_dir=\"./generated_src4CAP\",\n",
    "                              namespace=\"hana.ml\")\n",
    "hanagen.generate_artifacts(isof, model_position=True, cds_gen=False, tudf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows current path: /home/user/projects/teched2025-DA261/exercises/ex1\n",
    "!pwd\n",
    "\n",
    "# in case of windows systems\n",
    "#!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List artifacts in target directory\n",
    "!ls ./generated_src4CAP/OutlierAnalysis_HANA-CAP/db/src/\n",
    "\n",
    "# in case of windows systems\n",
    "#!dir .\\\\generated_src4CAP\\\\OutlierAnalysis_HANA-CAP\\\\db\\\\src\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./generated_src4CAP/OutlierAnalysis_HANA-CAP/db/src/hana-ml-base-pal-isolation-forest.hdbprocedure\n",
    "#!type .\\\\generated_src4CAP\\\\OutlierAnalysis_HANA-CAP\\\\db\\\\src\\\\hana-ml-base-pal-isolation-forest.hdbprocedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0aaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./generated_src4CAP/OutlierAnalysis_HANA-CAP/db/src/hana-ml-cons-pal-isolation-forest.hdbprocedure\n",
    "#!type .\\\\generated_src4CAP\\\\OutlierAnalysis_HANA-CAP\\\\db\\\\src\\\\hana-ml-cons-pal-isolation-forest.hdbprocedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc48bb8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Generating non-CAP, HANA HDI only artifacts for a native SAP HANA application"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b79d973e",
   "metadata": {},
   "source": [
    "from hana_ml.artifacts.generators.hana import HanaGenerator\n",
    "hg_HDI = HanaGenerator(project_name=\"OutlierAnalysis_HANA-HDI\", version='1', grant_service='', \n",
    "                       connection_context=myconn, \n",
    "                       outputdir=\"./generated_src4HDI\")\n",
    "hg_HDI.generate_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of later artifact generation, enable sql tracing\n",
    "myconn.sql_tracer.enable_sql_trace(False)\n",
    "myconn.sql_tracer.enable_trace_history(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc59bc",
   "metadata": {},
   "source": [
    "## Setting Isolation Forest parameter values with larger datasets (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf1b87",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithms is commonly regarded as an outlier detection technique is regarded as a technique also suitable to be applied to large datasets, like fraud detection on transactional data.\n",
    "- In such cases, the Isolation Forest outlier model could be trained on a represenative and large enough sample of the data. While the predict method with the trained outlier model can then applied to full data, incl. use of additional parallelization techniques.\n",
    "- Nevertheless, the adjustment and experimentation with Isolation Forest parameter values will be required to handle larger datasets. \n",
    "\n",
    "For larger datasets, increasing both max_samples and n_estimators when fitting the model can improve the accuracy of anomaly detection by capturing more diverse information and achieving a better consensus among trees. The model fitting then will certainly require more omputation time and resources.  \n",
    "Therefore, as the dataset grows, we might consider raising both max_samples and n_estimators. \n",
    "    - For max_samples, consider incrementally increasing it by a fraction (e.g. 10%) of the dataset size each step, provided that computational resources allow. \n",
    "    - For n_estimators, aim to gradually enhance it by 100 increments until reaching a threshold such as 1000. \n",
    "      However, the extent of this increase should be guided by computational constraints, which are difficult to predict without conducting experiments and validation.\n",
    "      \n",
    "Example, if you have a data set to analyze of 10.000.000 rows\n",
    "- you may consider to train the model on fraction of the data, e.g. let's say 10%, 1 million rows\n",
    "- Now think you seek to cover 100% of data utilized to build the model, then max_samples * n_estimators needs to at least calculate up to 1 million\n",
    "    - You could evaluate n_estimators=500 (build 500 trees), and max_samples=2000 (each tree to capture data of 2000 rows)\n",
    "    - If you switch bootstrap=True, row sampling happens with replacement, thus each row could be sampled multiple times and therefore you would require higher values for n_estimators and max_samples to avoid that rows are not sampled by any trees. The model itself would become more robust, as multiple trees would determine together if a data point is an outlier or not.\n",
    "    - If the data pattern is extremely diverse, the decision may grow very large, i.e. a very large decision tree-depth implying very big models requiring more resources to use, and possibly overfitted, too granular models. Remember, Isolation Forest tries to isolate outliers by their identification at shorter tree-depths in the tree. Therefore you should choose max_samples at max to ~50.000, which will lead to a tree_depth of ~16 (rule of thumb calculation Log(2, <max samples value>) <= 16).  \n",
    "    ROUND(Log(2, {datarows}/SAMPLES_PCT), 0) AS MAX_DEPTH,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80fe19",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Moreover, it is __strongly recommended to apply workload classes__ controlling the maximum resources to be consumed with a PAL-call, or multiple calls as a whole.\n",
    "- There are multiple ways an SAP HANA database administrator can setup the workload classed, mapping the constrained to users, certain database objects like the PAL procedures or applications. \n",
    "- For details see [workload class management documentation](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-administration-guide/managing-workload-classes-in-sap-hana-cloud-central?locale=en-US&version=LATEST)\n",
    "\n",
    "Within hana-ml scenarios, if not anyhow implicitly applied to the user and scenario, use of an existing workload class can be applied by setting it for the PAL method object. It is then attached with the anonymous SQL block generated by hana-ml and sent for each execution initiated by the object, untile disabled again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26542f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable_workload_class(workload_class_name)\n",
    "isof.enable_workload_class(\"PAL_AUTOML_WORKLOAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e649d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows of your data slice to analyze for outliers\n",
    "datarows=hdf_acdoca_slice_id.shape[0]\n",
    "datarows2=hdf_acdoca_slice_id.count()\n",
    "print(datarows, datarows2)\n",
    "print(f\"Table {datarows} has {hdf_acdoca_slice_id.count()} record(s).\")\n",
    "print(f'The data slice, dataframe result set has {datarows} and {hdf_acdoca_slice_id.count()} record(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6aa4c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's say we want to build a outlier-model for 1mio rows sample of our 10mio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ea052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Isolation Forest method class\n",
    "from hana_ml.algorithms.pal.preprocessing import IsolationForest\n",
    "\n",
    "# Creating our IsolationForest model object names \"isof\"\n",
    "#isof = IsolationForest(random_state=251104, n_estimators=500, max_samples=2000, bootstrap=False)\n",
    "\n",
    "# Or with Bootstap sampling applied\n",
    "isof = IsolationForest(random_state=251104, n_estimators=500, max_samples=8000, bootstrap=True)\n",
    "\n",
    "# Executing the fitting, i.e. the training of the Isolation Forest Outlier model\n",
    "isof.fit(data=hdf_acdoca_slice, features=outlier_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7a57c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The predict-task is a row-idenpendent task and thus various parallel invocation techniques can be applied.\n",
    "Beside the PAL \"massive\" data-parallel function implementations, which provide an optimized implementation group-by-value parallel, independent processing including optimized resources usage and batching of the parallelization task, PAL and the SAP HANA SQL engine provide additional parallelization techniques for such row- / data subset-independent processing tasks\n",
    "- PAL calls with a hint [parallel by partition](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/calling-pal-procedures-in-parallel-with-hint-parallel-by-parameter-partitions-calling-pal-procedures-in-parallel-with-hint-parallel-by-parameter-partitions-ed5807b?locale=en-US&version=LATEST), would invoke one predict function call for each SAP HANA cloud table partition in parallel \n",
    "- Futhermore, such PAL calls maybe parallelized using SQL patterns [MAP_REDUCE](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/calling-pal-procedures-in-parallel-with-map-reduce?version=LATEST&locale=en-US) or [MAP_MERGE](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/calling-pal-procedures-in-parallel-with-operator-map-merge?locale=en-US&version=LATEST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9caec",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's apply the parallel by partition hint with the predict-call\n",
    "- \"apply_with_hint\" allows the use of SQL hints with the anonymous SQL block generated by hana-ml and sent for execution\n",
    "- The parameter value p1 with PARALLEL_BY_PARAMETER_PARTITIONS refers to the 1st input table of the PAL procedure call, which is the input data table and its physical partitioning scheme to be used to determine the parallelization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846611c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the hint for the the PAL method object\n",
    "isof.apply_with_hint('PARALLEL_BY_PARAMETER_PARTITIONS(p1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_results = isof.predict(data=hdf_acdoca_slice_id, key='ID', features=outlier_features, \n",
    "                        contamination=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42274b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, disabling the hint again for the PAL method object\n",
    "isof.disable_with_hint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isof.get_predict_execute_statement())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b2301",
   "metadata": {},
   "source": [
    "## Setting Isolation Forest parameter values for each \"massive\" grouping set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d20efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data for illustration on two G/L Accounts\n",
    "filterSQL=f'\"G/L Account\" in (720000, 630000)'\n",
    "hdf_acdoca_slice_id.filter(filterSQL).head(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c18397",
   "metadata": {},
   "source": [
    "Applying parameters with massive, data-parallel Isolation Forest outlier analysis scenarios\n",
    "- Parameter value applied by general parameter, will be applied to all groups without any group-specific setting (e.g. n_estimators=101)\n",
    "- Group_params allows to set parameters for each individual gouping set by its group-id value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massive data-parallel Isolation Forest Outlier Analysis with group-specific parameter values\n",
    "from hana_ml.algorithms.pal.preprocessing import IsolationForest\n",
    "parallel_isof = IsolationForest(massive=True, random_state=2,  n_estimators=101, max_samples=100000,\n",
    "                                group_params={'720000': {'n_estimators':50, 'max_samples' : 10000}, \n",
    "                                              '630000': {'n_estimators':50, 'max_samples' : 10000}})\n",
    "\n",
    "filterSQL=f'\"G/L Account\" in (720000, 630000)'\n",
    "parallel_isof.fit(data=hdf_acdoca_slice.filter(filterSQL), group_key=\"G/L Account\", features=outlier_features)\n",
    "\n",
    "res, err = parallel_isof.predict(data=hdf_acdoca_slice_id.filter(filterSQL), key=\"ID\", group_key=\"G/L Account\", features=outlier_features,  contamination=0.05,   \n",
    "                                 group_params={'720000': {'contamination': 0.10}, \n",
    "                                              '630000': {'contamination':  0.025}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_isof.get_fit_execute_statement())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db331673",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Preparing the group_parameters as a python dict-variable and applying it to the method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mygroup_params=dict({'720000': {'contamination': 0.10}, '630000': {'contamination':  0.025}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb23cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massive with reason code\n",
    "res, err = parallel_isof.predict(data=hdf_acdoca_slice_id.filter(filterSQL), key=\"ID\", group_key=\"G/L Account\", features=outlier_features,  contamination=0.05,   \n",
    "                       group_params=mygroup_params\n",
    "                       ,show_explainer=True, explain_scope='outliers', top_k_attributions=5\n",
    "                      )\n",
    "display(res.sort('SCORE', desc=True).head(10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805554eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_isof.get_predict_execute_statement())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34042c3",
   "metadata": {},
   "source": [
    "## Model storage and retrieval of outlier Isolation Forest models (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e07b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.model_storage import ModelStorage\n",
    "MODEL_SCHEMA = '<your user schema | or different>' # HANA schema in which models are to be saved\n",
    "model_storage = ModelStorage(connection_context=myconn, schema=MODEL_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isof.model_.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ee805",
   "metadata": {},
   "outputs": [],
   "source": [
    "isof.name = 'IF_ACDOCA_OUTLIERMODEL'\n",
    "model_storage.save_model(model=isof) #if_exists='replace', if_exists='upgrade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8569bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_storage.list_models(display_type='simple')) #display_type: 'complete', 'simple', 'no_reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve model\n",
    "isof_reloaded = model_storage.load_model(name='IF_ACDOCA_OUTLIERMODEL', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc773030",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = isof_reloaded.predict(data=acdoca_hdf, key='ID', features=outlier_features,\n",
    "                       contamination=0.05)\n",
    "print(out.head(3).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage.delete_model(name='IF_ACDOCA_OUTLIERMODEL', version=1)\n",
    "#model_storage.delete_models(name=model.name)\n",
    "#model_storage.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_storage.list_models(display_type='simple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf32e36-8c24-4bf5-bd43-4cc2aeef6487",
   "metadata": {},
   "source": [
    "## Outlier data generation (optional)\n",
    "Instructions on how to generate sample outlier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021baac1-b125-4874-b603-6cdfc75a37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "company_codes = ['CC01']\n",
    "gl_accounts = [str(x) for x in range(400000, 800000, 10000)]\n",
    "profit_centers = ['PC001', 'PC002', 'PC003']\n",
    "cost_centers = ['C101', 'C102', 'C103']\n",
    "functional_areas = ['FA01', 'FA02']\n",
    "business_areas = ['BA01', 'BA02', 'BA03']\n",
    "segments = ['S1', 'S2', 'S3']\n",
    "dc_indicators = ['S', 'H']\n",
    "doc_types = ['SA', 'SB', 'SC', 'SD']\n",
    "tx_types = ['TA01', 'TA02', 'TA03']\n",
    "fin_types = ['P+L Statement', 'Balance Sheet Asset', 'Balance Sheet Liability', 'Equity']\n",
    "data = []\n",
    "for _ in range(500):\n",
    "   amount = round(random.uniform(-20000, 20000), 2)\n",
    "   data.append([\n",
    "       random.choice(company_codes),\n",
    "       random.choice(gl_accounts),\n",
    "       random.choice(profit_centers),\n",
    "       random.choice(cost_centers),\n",
    "       random.choice(functional_areas),\n",
    "       random.choice(business_areas),\n",
    "       random.choice(segments),\n",
    "       random.choice(dc_indicators),\n",
    "       random.choice(doc_types),\n",
    "       random.choice(tx_types),\n",
    "       random.choice(fin_types),\n",
    "       abs(amount),\n",
    "       amount\n",
    "   ])\n",
    "df = pd.DataFrame(data, columns=[\n",
    "   'Company Code', 'G/L Account', 'Profit Center', 'Cost Center',\n",
    "   'Functional Area', 'Business Area', 'Segment', 'Debit/Credit',\n",
    "   'Accounting Document Type', 'Transaction Type', 'Financial Account Type',\n",
    "   'Amount (USD)', 'Amount (Transaction)'\n",
    "])\n",
    "df.to_csv('acdoca_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.dataframe import create_dataframe_from_pandas\n",
    "import pandas as pd\n",
    "acdoca_hdf = dataframe.create_dataframe_from_pandas(\n",
    "        myconn,\n",
    "        df,\n",
    "        table_name=\"ACDOCA\",\n",
    "        force=True,\n",
    "        replace=True,\n",
    "        drop_exist_tab=True\n",
    "        )\n",
    "print(acdoca_hdf.select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(acdoca_hdf.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
