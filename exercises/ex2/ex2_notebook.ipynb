{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d88e7e-f3f6-4191-9950-4bbedef19861",
   "metadata": {},
   "source": [
    "# Exercise 2 - Jupyter Notebook \n",
    "## Analyzing consumer complaints using text embeddings and machine learning\n",
    "SAP TechEd 2025, Hands-On Workshop: DA261 - Unlocking AI-driven insights from your business data in SAP HANA Cloud\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a2d7b-1ede-4efa-957b-fc05c0a4bbe1",
   "metadata": {},
   "source": [
    "### Understanding the exercise scenario [0:30s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55273c5-7199-410e-9de0-c1b6b53dfe43",
   "metadata": {},
   "source": [
    "In this exercise, you will explore how to __classify and process consumer complaints texts__ using \n",
    "- __text analysis for sentiment detection__, \n",
    "- __similarity search__ with __text embeddings__ and \n",
    "- __AutoML__ techniques to build a __consumer complaints classification machine learning model__,  \n",
    "- thus __unlocking__ the __semantic understanding of text data__ as text embedding feature with machine learning models. \n",
    "\n",
    "<br>\n",
    "\n",
    "The __classification of incoming service requests__ is a __common machine learning use case__. This exercise illustrates, how such a use case can be implemented using __out-of-the-box capabilities from the SAP HANA Cloud database__, inluding a text embedding model to generate embedding vector, a vector engine to store the generated text embeddings, similarity search functions with vector columns and machine learning algorithms from Predictive Analysis Library (PAL) in the database AI engine.\n",
    "\n",
    "The data used in this scenario, derives from the __consumer complaint database__, from the US government Consumer Financial Protection Bureau.  \n",
    "Details regarding data license, download and import instruction are given [Appendix section](##loading-the-data-sample).\n",
    "<br>\n",
    "\n",
    "Now let's get started with the exercise!\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b07eb-11ef-41d6-a9db-2c0ffe1e536f",
   "metadata": {},
   "source": [
    "## Ex. 2.0 - Connect to your SAP HANA Cloud instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f411a-9d0f-4987-a1d8-b65e686b100f",
   "metadata": {},
   "source": [
    "Throughout this exercise, we will be using the __Python Machine Learning client for SAP HANA__, as your reference to all its functions and capabilities see the [hana-ml documentation](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/change_log.html). The current version released with SAP HANA Cloud 2025 Q3 release is 2.26.\n",
    "- The python package hana-ml in general allows to script in python, while SQL code is generated on-the-fly and directly passed to a connected SAP HANA database system for execution.\n",
    "    - It allows to access and prepare data by means of a HANA dataframe, a python object holding a SQL select query. Many methods are provided to be used with the HANA dataframe, changing the SQL select query behind the scenes.\n",
    "    - As its core, it provides methods to apply AI functions (algorithms from the Predictive Analysis Library PAL and Automated Predictive Library APL) to the data prepared with a HANA dataframe, designed to apply all the processing within the SAP HANA database.\n",
    "- The python package is delivered with the general SAP HANA Client, in addition the latest can always be found in the pypi public repository at https://pypi.org/project/hana-ml/\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50889bc-164a-4603-9328-76c5c8b8bf9f",
   "metadata": {},
   "source": [
    "In Python, installed packages require to be imported into the session so they can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0822b3-6c94-499b-a9ac-85033f8030c1",
   "metadata": {},
   "source": [
    "### Step 0: Establish and check connection [2:5s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98fc5e-2925-45ea-8bcc-7adc83cb5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Python Machine Learning client library for SAP HANA and review the current client version\n",
    "import hana_ml\n",
    "print(hana_ml.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265744b-61a9-47d4-b01f-0706aed2a5df",
   "metadata": {},
   "source": [
    "Running the referenced script prepared in the Getting Started section to connect to the SAP HANA CLoud database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34658c-c04d-4223-8b4b-1dae43d889bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../ex0/ex0_2-check_setup.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9a97a-deda-47b6-ba6f-52dbddb4ddb4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Ex. 2.1 - Exploring Consumer Complaints Data\n",
    "### Step 1: Create the HANA dataframe for the consumer complaints data [5:60s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b634ae7-88b8-4346-afb7-a00b223fa0c3",
   "metadata": {},
   "source": [
    "__Introduction to SAP HANA dataframes__\n",
    "- The [HANA dataframe](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/hana_ml.dataframe.html#module-hana_ml.dataframe) represents a database query as a dataframe in python, comarable to a pandas dataframe. Most operations are designed to not bring data back from the database unless into the python runtime explicitly requested. \n",
    "- SAP HANA dataframes can be created\n",
    "    - based on table, view, calculation view (incl. parameters), SQL statement incl. multi-statement\n",
    "    - or create from pandas dataframe or sparkd dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45165732-2bce-4205-bfd2-961578d3ee5a",
   "metadata": {},
   "source": [
    "Based on the [ConnectionContext-object](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_2_QRC/en-US/hana_ml.dataframe.html#hana_ml.dataframe.ConnectionContext) __\"myconn\"__, a child of the dataframe-class, we are using the table-method to create the initial HANA dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53985811-81a7-4b48-8751-bb506fa37410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the connection using \"myconn\"\n",
    "myconn.connection.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08227f1-5b41-4bb4-8bfc-ef5514bf5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a HANA dataframe in Python against the HANA Cloud table, data is preloaded into the system based on the details in /Appendix/Loading the data sample-section\n",
    "consumercomplaints_hdf = myconn.table(\"CONSUMER_FINANCIAL_COMPLAINTS\", schema=\"DA261_SHARE\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216705ed-70f2-4f25-9c18-1669c73b73cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "Explore what a HANA dataframe object is within the python evironment. The python methods print() or display() present the output of executed python commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6624e50-25ce-4f18-87c9-eabcf74356a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand what is the HANA dataframe\n",
    "print(consumercomplaints_hdf)\n",
    "print(consumercomplaints_hdf.select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7df46-140f-4196-8142-836a5328c2b9",
   "metadata": {},
   "source": [
    "<br>\n",
    "Understand the data structure of the query set underlying the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3d26a-b496-4c1e-adfd-3f469966b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the data structure of the query set underlying the dataframe, using the shape, columns and dtypes() methods\n",
    "print(consumercomplaints_hdf.shape, '\\n')\n",
    "print(consumercomplaints_hdf.columns, '\\n') \n",
    "display(consumercomplaints_hdf.dtypes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f04ca7-d578-4f14-a8f1-b97c52c47832",
   "metadata": {},
   "source": [
    "As the shape method output indicates, the table for the purpose of this session has 128330 rows and 19 columns.  \n",
    "The column __ConsumerComplaintNarrative__ has type NCLOB, and the __ComplaintNarrative_EMBEDDING__ has type REAL_VECTOR with 768 dimensions.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfd875-13df-419b-b7dd-1439b5674d4e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For an __initial view at the data__, the __head()__- in conjunction with the __collect()-method__ can be used.\n",
    "\n",
    "- Only when the collect()-method is used, data is brought back into the python envrionment. __Do not__ use __collect() without__ any further __filtering methods__ applied.\n",
    "- The head method adds the TOP N predicate to the SQL select clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38108f50-c450-41f4-a5b7-53abf7ba5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on the TOP 5 rows, and show them in python\n",
    "display(consumercomplaints_hdf.head(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7a3ce-8726-4f5b-b8c2-5f8ef857df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a custom column list using the select-dataframe method\n",
    "\n",
    "# Maximize row display beforehand\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None) \n",
    "\n",
    "# Display columns 'ConsumerComplaintNarrative', 'ComplaintNarrative_EMBEDDING'\n",
    "display(consumercomplaints_hdf.select('ConsumerComplaintNarrative', 'ComplaintNarrative_EMBEDDING').head(2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a022232-1b32-40f0-a420-e26af6f338a9",
   "metadata": {},
   "source": [
    "<br>\n",
    "The dataframe and its method applied, is only a SQL query statement. So how does it look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b469d95-81b3-4d68-ba14-d776547e077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(consumercomplaints_hdf.select('ConsumerComplaintNarrative', 'ComplaintNarrative_EMBEDDING').head(5).select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0d12e-c94d-40f8-9859-e167ef0ec266",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Step 2: Explore consumer complaints data [5:120s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3872b59-349c-4a7e-8e3b-0f6ac668d7ac",
   "metadata": {},
   "source": [
    "Let's look as some exemplary HANA dataframe methods for aggregation, filtering and data exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae419f03-8e02-4c74-9682-e47c2c3e7f16",
   "metadata": {},
   "source": [
    "The __describe()__-method, provides insightful details about column value distribution and statistics, especially for numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44420631-9b39-4fbc-934c-2e2f47a30d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calling the describe()-method, we use the drop-method to exclude the unsupported NCLOB-type columns from the analysis\n",
    "consumercomplaints_hdf.drop(\"ConsumerComplaintNarrative\").describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5886b-83be-4eb4-8630-87c53b049161",
   "metadata": {},
   "source": [
    "Take notice of unique value counts, nulls and not null-counts.  \n",
    "The complaintID is the only numeric (Integer) column, however as an ID column really doesn't suite for looking at the column value statistics.  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1658ab-f155-4655-95d2-f045637a2d3a",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>\n",
    "__What are the key issues categories, consumer complain about?__  \n",
    "Let's create a new HANA dataframe using the __sql()-method__, wich allows to apply multi-line sql-statements as the initial HANA dataframe query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faec75c-10f3-4d9b-b518-a60e2467cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a SQL query, filtering for issue categories which count more than 1000 complaints. For illustraion, we didn't apply a having-clause select query.[result: 108903]\n",
    "cc_key_issues_hdf=myconn.sql(\"\"\"\n",
    "Select * from \"DA261_SHARE\".\"CONSUMER_FINANCIAL_COMPLAINTS\" as CC \n",
    "    WHERE CC.\"Issue\" in (Select distinct \"Issue\" \n",
    "                         from (Select \"Issue\", Count(\"ComplaintID\") as N from \"DA261_SHARE\".\"CONSUMER_FINANCIAL_COMPLAINTS\"\n",
    "                               Group by \"Issue\" \n",
    "                               having Count(\"ComplaintID\") > 1000 ))\n",
    "\"\"\")\n",
    "\n",
    "print('Dataframe shape: ', cc_key_issues_hdf.shape, '\\n')\n",
    "\n",
    "display(cc_key_issues_hdf.drop(\"ComplaintNarrative_EMBEDDING\").head(2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c305ff-d0de-4b21-81d9-679dcd04eea5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For getting an __aggregated view__, on the __number of issues per issue category__ with >1000 counts, let's use a __bar-chart visualization__ with the HANA dataframe\n",
    "- The hana-ml visualizations automatically push-down alls the aggregation queries required into the SAP HANA database, only required result sets are provided to the visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3fe3b-8266-4d69-824a-685554ba39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar-chart visualization\n",
    "from hana_ml.visualizers.eda import EDAVisualizer \n",
    "eda = EDAVisualizer(enable_plotly=True)\n",
    "\n",
    "fig, bar = eda.bar_plot(data=cc_key_issues_hdf, \n",
    "                        column=\"Issue\", \n",
    "                        aggregation={'ComplaintID':'count'}, \n",
    "                        width=1200, height=600, \n",
    "                        title=\"Count of complaints by key Issue categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3adfb8-9803-41fd-919f-4b37ae295065",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "__How do financial companies respond to consumer complaints?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06824f26-04c1-43a6-87e6-220541fc88fa",
   "metadata": {},
   "source": [
    "Explore complaints by __Company public response__, looking at the full original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3205139-7b3b-48ab-aa7c-8fca35521e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar-chart visualization\n",
    "fig, bar = eda.bar_plot(data=consumercomplaints_hdf, \n",
    "                        column=\"CompanyPublicResponse\", \n",
    "                        aggregation={'ComplaintID':'count'}, \n",
    "                        width=1200, height=400, orientation='h',\n",
    "                        title=\"Count of ComplaintID by CompanyPublicResponse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fedbd-c239-4297-9c54-1d87bc77b025",
   "metadata": {},
   "source": [
    "Explore complaints by __Company response category__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569e4ae-82c8-491b-b0c7-b7900a988923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar-chart visualization\n",
    "fig, bar = eda.bar_plot(data=consumercomplaints_hdf, \n",
    "                        column=\"CompanyResponseToConsumer\", \n",
    "                        aggregation={'ComplaintID':'count'}, \n",
    "                        width=1200, height=300, orientation='h',\n",
    "                        title=\"Count of complaints by CompanyResponseToConsumer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45cdb0-b987-4e45-8a73-362b3c1fd707",
   "metadata": {},
   "source": [
    "As we can see, financial consumer products companies responded to a significant amount to consumer complaints with a __monetary relief__. It might be of crucial interest for companies to earyl on detect such consomer complaints wich might lead towards a monetary consumer complaint compensation. We will come back to this scenario.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127b72d",
   "metadata": {},
   "source": [
    "## Ex. 2.2 Analyze consumer sentiment using text analysis (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902874d-fb47-47d6-b314-11c9dc8808ce",
   "metadata": {},
   "source": [
    "### Step 3: Exploring the sentiment of consumer complaints [6:120s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45ee8d-dbd2-4d2f-906f-b7ee2104e02f",
   "metadata": {},
   "source": [
    "It is of key interest for financial product companies, not to stay in dispute with customers. In the data analyzed in the exercise, the consumer financial protection bureau discontinued the [Consumer disputed?](https://cfpb.github.io/api/ccdb/fields.html), which gave a clear customer sentiment-category regarding the dispute status.  \n",
    "From the __complaint narrative__ itself (or of course from direct customer interaction communication), financial product or services companies often __apply sentiment analysis__techniques to __detect trends__, especially in publicly visible communications, forum, chatting or evaluation threads. Is there a negative sentiment trend on our product or company, could be a common question to be answered.  \n",
    "<br>\n",
    "__Sentiment analysis__ as a technique in Natural Language Processing (NLP) and text analysis, is still applied commonly to detect respective information early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34569051-5efa-4a3a-8bd7-d1249188a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look, how many complaints had been classified as disputed, using the dataframe agg()-method for an aggregation analysis\n",
    "consumercomplaints_hdf.agg([('count','ComplaintID','n_COMPLAINTS')] ,group_by = ['ConsumerDisputed']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc2c78-bd75-4a85-8bcf-75e86c6ba5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish, take a look at the HANA dataframe SQL query statement again\n",
    "#consumercomplaints_hdf.agg([('count','ComplaintID','n_COMPLAINTS')] ,group_by = ['ConsumerDisputed']).select_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff668df-bcb7-4137-a596-0f1f8ed9f817",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Let's try to understand, __which companies experienced larger numbers of complaints qualified by consumers as still disputed__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec052edd-e995-45c2-839b-d32348a1ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new, temporary dataframe where we apply the filter()-method using a SQL where-clause expression\n",
    "hdf_tmp=consumercomplaints_hdf.filter('\"ConsumerDisputed\" = \\'Yes\\'')\n",
    "\n",
    "# Then count the remaining complaints by company using the agg()-method\n",
    "hdf_tmp.agg([('count','ComplaintID','n_COMPLAINTS')] ,group_by = ['Company']).sort('n_COMPLAINTS', desc=True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ce540-d74f-4f45-940a-0494e4690be8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Let's __filter the complaints for a company__, with a __significant number of complaints voiced to be in dispute__ by the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5c7ba-d2f5-49b8-abb7-cafc44efa9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new, temporary dataframe where we apply the filter()-method using a SQL where-clause expression\n",
    "hdf_tmp=consumercomplaints_hdf.filter('\"ConsumerDisputed\" = \\'Yes\\' AND \"Company\" = \\'UNITED SERVICES AUTOMOBILE ASSOCIATION\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b19c5f-3389-49de-86a0-ea8ebd902a7f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, further __prepare__ the hana dataframe for the __sentiment analysis__  \n",
    "- Restrict the columns of interest using the select()-method  \n",
    "- Adding a __new column_ within the select()-method: ('<valid SQL value expression>', '<Column Name>'), in our case we seek to add a LANGUAGE column for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d2136-9078-402c-be91-dd86b726c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "hdf_tmp_sentiment=hdf_tmp.select('ComplaintID', 'ConsumerComplaintNarrative', 'CompanyPublicResponse', 'CompanyResponseToConsumer', 'Issue')\n",
    "\n",
    "# Add a new column using a SQL column value expression, here adding a static string 'en'\n",
    "hdf_tmp_sentiment=hdf_tmp_sentiment.select('*', ('\\'en\\'', 'LANGUAGE'))\n",
    "\n",
    "hdf_tmp_sentiment.head(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d075467-1d45-48d2-a532-41c127747b8f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Apply the __sentiment analysis__ method from the __text analysis__ function in __hana-ml__.\n",
    "- note, the text analysis functions in SAP HANA Cloud are executed by the NLP services, which require to be activated during configuration of the SAP HANA cloud instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003c7fc-ddab-4424-89b7-25841fe85017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the sentiment for the first 50 complaints\n",
    "hdf_tmp_sentiment=hdf_tmp_sentiment.select('ComplaintID',  'ConsumerComplaintNarrative', 'LANGUAGE').head(50)\n",
    "\n",
    "# Load the sentiment analysis function module and run the analysis for all sentiment areas, incl. document- and sentence-level sentiment\n",
    "# Results get captured in the ordered list of dataframe, we named: doc_sentiment, sentence_sentiment, phrase_sentiment, sentences, extra\n",
    "from hana_ml.text.ta import sentiment_analysis\n",
    "doc_sentiment, sentence_sentiment, phrase_sentiment, sentences, extra = sentiment_analysis(data=hdf_tmp_sentiment, thread_ratio=0.5, timeout=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76316d38-e416-4aa0-8a3a-ac496554142e",
   "metadata": {},
   "source": [
    "![](./images/ta_sentiment-classes.png)\n",
    "<!--- <img src=\"./images/ta_sentiment-classes.png\" width=400 height=150 /> --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5481e6-7021-4eda-b437-20d7cde37f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for document sentiment analysis\n",
    "display(doc_sentiment.sort('DOC_SENTIMENT_MAGITUDE', desc=True).head(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78891a-26c5-4212-bc95-a05d634e9977",
   "metadata": {},
   "source": [
    "Identify the ComplaintID, for the complaint showing the biggest document sentiment magnitude and explore it's sentence-level sentiment\n",
    "- Set the Identified ComplaintID into the filterSQL-variable as an additional in-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650db31-9ecd-46b1-82cf-6d6c3de41399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's further investage sentence-level sentiment for complaints\n",
    "\n",
    "filterSQL=f'\"ComplaintID\" in (9999999, <ComplaintID-value>)'  \n",
    "\n",
    "display(sentence_sentiment.filter(filterSQL).select('SENTENCE_ID', 'CONTENT', 'SENTIMENT_LABEL', 'SENTIMENT_SCORE', 'SENTIMENT_MAGNITUDE').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef517824",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 2.3 Search in consumer complaints narratives using similarity search (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0068a66-53bc-4256-afda-429d6f3c1e91",
   "metadata": {},
   "source": [
    "### Step 4: Explore consumer complaints narratives using Text Embeddings and similarity search [6:180s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbbac5-b83c-4ce8-ac39-6b48e512f9d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we've seen earlier, the initially provide data already contains a text embedding column. See the Appendix section on embeddings how those have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba62373-41b2-499c-bd5e-46b9adaacba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's review the original dataframe with complaint text and the generated text embedding column\n",
    "filterSQL=f'\"ComplaintID\" in (9999999, 5033663)'  \n",
    "display(consumercomplaints_hdf.select('ComplaintID','ConsumerComplaintNarrative', 'ComplaintNarrative_EMBEDDING').filter(filterSQL).head(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a408b3-c3ec-4cbc-98bb-2e2a3d6f3fae",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For our better understanding, let's __create a text embedding__ quickly using the __VECTOR_EMBEDDING-SQL function__ in a multi-line SQL dataframe query!  \n",
    "Do the text embedding vectors look like, compared to the previous cell output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152ed5c-de7f-4883-89d7-bf1cd6103ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the ComplaintID 5033663 and use the VECTOR_EMBEDDING-SQL function applying the latest text embedding model in SAP HANA Cloud (SAP_GXY.20250407)\n",
    "hdf_tmp_textembedding = myconn.sql(\n",
    "\"\"\"\n",
    "SELECT \"ComplaintID\", \"ConsumerComplaintNarrative\",\n",
    "\t\tVECTOR_EMBEDDING(\"ConsumerComplaintNarrative\", 'DOCUMENT','SAP_GXY.20250407') AS \"NEW_TEXTEMBEDDING_VECTOR\"\n",
    "\tFROM \"DA261_SHARE\".\"CONSUMER_FINANCIAL_COMPLAINTS\" \n",
    "    WHERE \"ComplaintID\"=5033663\n",
    "\t;\n",
    "\"\"\"\n",
    ")\n",
    "display(hdf_tmp_textembedding.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e69000-ddb2-4769-9487-b613266a5252",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "For the __similarity search scenario__, now let's __look at complaints__, where the financial product or service __companies__ responded with __monetary relief__.  \n",
    "We seek to explore, if we had been __victim of identity theft__ and complain respectively, would we find __similar complaints where the company responded with monetary relief__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23819c-7d93-4de8-8576-ad9b272b798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter complaints closed with monetary relief and select columns of interest\n",
    "hdf_tmp_simsearch=consumercomplaints_hdf.filter('\"CompanyResponseToConsumer\" in (\\'Closed with monetary relief\\')') \n",
    "hdf_tmp_simsearch=hdf_tmp_simsearch.select('ComplaintID','ConsumerComplaintNarrative', 'ComplaintNarrative_EMBEDDING')\n",
    "\n",
    "# Display a sample of complaint texts 'closed with monetary relief'\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None) \n",
    "display(hdf_tmp_simsearch.select('ComplaintID','ConsumerComplaintNarrative').head(3).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8c1cd-6b4a-4289-91f8-ed7cf35305a2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, let's use __similarity search__ for complaints by consumers who received a monetary compensation, similar to our __inquiry text__ about  __identity theft__   \n",
    "- note, for text embedding vector similariy search our __inquiry text__ get's __transformed into a text embedding__ using the __embed_query()-method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15506bf-5c62-4330-a3f9-11a8408c0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, search for complaints by consumer victims similar to the topic of identity theft expressed in our complaint text\n",
    "search_sentence = \"\"\"As I have reported before, I am a victim of identity theft that happenned during the last black friday sales and I have been affected ever since. \n",
    "                     I have called the companies and spoke with them in length that these items are not mine. \n",
    "                     I have done everything I can to have these items removed. \n",
    "                     They are effecting my everyday life I literally have lost my trust in credit card and internet security over this identity theft. \n",
    "                     I expect monetary compensation for those fraudulent credit cards transaction caused by identity theft.\"\"\"\n",
    "search_sentence_embedding = myconn.embed_query(search_sentence)\n",
    "\n",
    "# Let's see if the embedding or our query sentence worked? \n",
    "# Note, the slice-method here just helped with the cut-off for print display of the embeddings after 24 elements of the vector\n",
    "print(search_sentence_embedding[slice(24)]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7210b-a89d-4280-a456-0f99683b3596",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Executing the Vector Engine similarity search using the search_by_similarity()-dataframe method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aa9e3-ba56-4cad-9ddf-4a8d4fe28904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the Vector Engine similarity search using the search_by_similarity()-dataframe method\n",
    "# Choose between similarity_function 'L2DISTANCE' | 'COSINE_SIMILARITY'\n",
    "hdf_tmp_simsearch.sort_by_similarity(\"ComplaintNarrative_EMBEDDING\", query=search_sentence, model_version='SAP_GXY.20250407', similarity_function='COSINE_SIMILARITY'\n",
    "                                   ).select('ComplaintID', 'ConsumerComplaintNarrative', 'SIMILARITY').head(10).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47699c44-3fd5-4bfb-b7e0-ffabf191766f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "How does such a __vector similarity search query__ look like __SQL__?   \n",
    "Would it work to pass an __inquiry__ in a __different language__ but __still get semantically similar results__ from the text embeddings searched again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086bd44-b0dc-4762-bb6c-42df0d36bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for complaints related to \"Je suis victime d'usurpation d'identité et de fraude à la carte bancaire. Je réclame donc une indemnisation.\"\n",
    "# or compose a search text youself \"I'd like to report .... \"  > let's see if you get high similarity score results with your query.\n",
    "hdf_tmp_simsearch_sql = myconn.sql(\n",
    "\"\"\"\n",
    "SELECT \"ComplaintID\", \"Product\", \"ConsumerComplaintNarrative\",\n",
    "\t\tCOSINE_SIMILARITY(\"ComplaintNarrative_EMBEDDING\", \n",
    "                          VECTOR_EMBEDDING('Je suis victime de usurpation de identité et de fraude à la carte bancaire. Je réclame donc une indemnisation.', 'DOCUMENT','SAP_GXY.20250407')) AS SIM\n",
    "\tFROM \"DA261_SHARE\".\"CONSUMER_FINANCIAL_COMPLAINTS\" \n",
    "\tORDER BY SIM DESC\n",
    "\tLIMIT 10;\n",
    "\"\"\"\n",
    ")\n",
    "display(hdf_tmp_simsearch_sql.collect()) #\"Company\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad2499-e265-40da-873d-890e92fae537",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Sections 2.4 - Predicting monetary relief-response to consumer complaints using AutoML classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c776ec7-9fae-4d5f-a5df-95582e3a43fe",
   "metadata": {},
   "source": [
    "To use of machine learning models __classify__ incoming customer emails, complaints, etc. or service requests or alike has been a common use case scenario.  \n",
    "With the recent emergence of __text embedding__ models __unlocking the semantic understanding of text data__ and __machine learning algorithm enhancements__ to be capable to process such rich and high-dimensional embedding vectors as feature, this rich understanding of text data now unlocks the potential to build even better classification models for those use case.  \n",
    "- Latest machine learning algorithms can process a mix of classic numeric or catgorial attributes along with vector-type features. \n",
    "- Pre-processing techniques in machine learning, like principal component analysis of vectors allow the extraction of key information into a lower dimensional space, e.g. from vectors with 768 or far more dimensions to 64, 128 or 256 dimensions with only little to neglectable precision loss by the machine learning model while gaining much better processing times. \n",
    "\n",
    "This exercise illustrates, how such a use case can be implemented using __out-of-the-box capabilities from the SAP HANA Cloud database__, inluding a text embedding model to generate embedding vector, a vector engine to store the generated text embeddings and machine learning algorithms from Predictive Analysis Library (PAL) in the database AI engine.\n",
    "- PAL algorithms enabled to process vector data incl. AutoML, Hybrid Gradient Boosting Trees (HGBT), Multi-Target MLP (MT_MLP), KMEANS, HDBSCAN, Vector PCA, and many more.\n",
    "- note the SAP HANA Cloud text embedding models is processed by the NLP services, use of those requires to be activated during configuration of the SAP HANA cloud instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595f6dd-c7f8-482d-951d-3a79847fc583",
   "metadata": {},
   "source": [
    "### Step 5: Data selection and preparation [5:90s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9dc21-ad94-49fb-affe-55d14536ac40",
   "metadata": {},
   "source": [
    "For the exemplary __machine learning classification__ scenario, to __predict__ and thus early alert on a potentially required __monetary relief-response to a customer complaint__,  \n",
    "we are seeking to select the data for __a company__ with a __high proportion of complaint responses with monetary relief__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0f89c-7231-4c4a-a268-1850b3146220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, to reduce the relevant data, we are filtering out complaints where company responded about acting as authorized by contract or law\n",
    "hdf_tmp=consumercomplaints_hdf.filter('not \"CompanyPublicResponse\" in (\\'Company believes it acted appropriately as authorized by contract or law\\')')\n",
    "\n",
    "# Count the remaining complaints per company, mapped into the dataframe: hdf_complaints_company\n",
    "hdf_complaints_company=hdf_tmp.agg([('count','ComplaintID','n_complaints')] ,group_by = ['Company']).sort('n_complaints', desc=True)\n",
    "\n",
    "# Count the complaints responded to with monetary relief per company, mapped into in the dataframe: hdf_complaints_monetary_company\n",
    "hdf_complaints_monetary_company=hdf_tmp.filter('\"CompanyResponseToConsumer\" = \\'Closed with monetary relief\\'').agg([('count','ComplaintID','n_complaints')] ,group_by = ['Company'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca8977-4e38-46be-ac7d-64a5166e0c34",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Join and calculate percentage of complaints respondet to with monetary relief, map the result in the dataframe: hdf_pct_monetary  \n",
    "New dataframe-functions introduced in this step are\n",
    "- rename_columns({'<name>': 'new_name'}, {..}, ...)\n",
    "- sort for sorting the resulting query sets by a column's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177946a-95d4-47ab-97f0-476df99195b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes, rename column n_complaints as it is in both join-dataframes\n",
    "hdf_joined=hdf_complaints_company.set_index(\"Company\").join(hdf_complaints_monetary_company.rename_columns({'n_complaints': 'n_complaints_monetary_relief'}).set_index(\"Company\"))\n",
    "\n",
    "# Calculate the percentage of complaints respondet to with monetary relief, map the result in the dataframe hdf_pct_monetary\n",
    "hdf_pct_monetary=hdf_joined.select('Company', 'n_complaints', 'n_complaints_monetary_relief', ('(\"n_complaints_monetary_relief\"/\"n_complaints\")*100', 'PCT_MON'))\n",
    "\n",
    "# Sort and filter the final results set\n",
    "hdf_pct_monetary.filter('\"n_complaints_monetary_relief\" > 50').sort('PCT_MON', desc=True).head(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e8e1e-a60d-401f-a8bf-8f78fb60ac28",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "The __United Services Automobile Association (USAA)__ has a decent number of overall complaints and a significant proportion responded with monetary relief, well suitable for this hands-on exercise. The USAA is a private American financial services and insurance company that provides a range of banking, insurance, investment, and retirement solutions.   \n",
    "- so let's filter for the USAA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3de1cd-dd61-41b0-bcdf-40b192bf6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the filtered data using the describe()-method, looking at columns for missing values, and columns suitable to include in the classification model\n",
    "hdf_tmp=hdf_tmp.filter('\"Company\" = \\'UNITED SERVICES AUTOMOBILE ASSOCIATION\\'')\n",
    "\n",
    "# Recall - we need to exclude NCLOB columns from the describe-analysis\n",
    "hdf_tmp.drop(\"ConsumerComplaintNarrative\").describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d7d39-07b4-495b-9452-2195e45264a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we start to prepare our HANA dataframe to build the classificaton model, let's drop column with lot's of NULL values or drop respective rows.  \n",
    "- Lot's of nulls in columns 'SubProduct', 'SubIssue','State', 'ConsumerDisputed'\n",
    "- Only a small number of nulls in column 'State'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4f98a-9968-43af-88b4-5e3b1997df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As explored with the describe-method, we want to drop columns for the classification task with lots of NULL values or drop respective rows\n",
    "hdf_tmp=hdf_tmp.select('ComplaintID', 'Product', 'Issue', 'State', 'ZipCode', 'ComplaintNarrative_EMBEDDING', 'CompanyResponseToConsumer')\n",
    "hdf_tmp=hdf_tmp.filter('not \"State\" is NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a96183-6dc8-4ec0-9cfe-6dd622f773c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename classification target column to LABEL and replace ' ' with '_' in LABEL values for later processing\n",
    "hdf_tmp=hdf_tmp.select('ComplaintID', 'Product', 'Issue', 'State', 'ZipCode', 'ComplaintNarrative_EMBEDDING', ('REPLACE(\"CompanyResponseToConsumer\", \\' \\', \\'_\\')', 'LABEL'))\n",
    "\n",
    "print(hdf_tmp.columns)\n",
    "print(hdf_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1fbc0-9166-48bb-b08b-eff46fb448a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review target LABEL value distribution\n",
    "hdf_tmp.agg([('count','ComplaintID','n_complaints')] ,group_by = ['LABEL']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d50d09-519f-4432-a75d-4cdcf1ee4c0e",
   "metadata": {},
   "source": [
    "Now we are almost ready to build the classification model.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506f0b4-c3d3-4273-ad8c-99fee9871d69",
   "metadata": {},
   "source": [
    "### Step 6: Consumer complaints Text Embedding vector dimension reduction [3:60s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433a513-aa8e-4982-80f0-7067ce23ece3",
   "metadata": {},
   "source": [
    "As described earlier, due to high dimensionality of vector data, it may impact the performance of downstream algorithms significantly, hence techniques to extract as much \"information\" from the very high dimensional space, into a lower dimensional space should be considered. One such dimension reduction technique is Principle Component Analysis, supported in the Predictive Analysis Library (PAL) to process vector input data in the functions Vector-PCA and Categorial-PCA.  \n",
    "Let's use Vector-PCA to reduce the dimensionality from a 768 dimension text embedding-vector to a 64 principal component-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df319759-ecdd-448a-8a0b-5b10e1ddc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the Text Embedding vector dimensions from 768 to a vector of 64 principal component dimensions\n",
    "from hana_ml.algorithms.pal.decomposition import VectorPCA\n",
    "vecpca = VectorPCA(n_components=64)\n",
    "hdf_pcavectors = vecpca.fit_transform(data=hdf_tmp.select('ComplaintID','ComplaintNarrative_EMBEDDING'), key='ComplaintID')\n",
    "\n",
    "hdf_pcavectors=hdf_pcavectors.rename_columns({'SCORE_VECTOR': 'ComplaintNarrative_PCA_VECTOR'})\n",
    "print(hdf_pcavectors.shape, '\\n')\n",
    "print(hdf_pcavectors.select_statement, '\\n')\n",
    "print('Vector-PCA runtime is: ', vecpca.runtime, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0604a-9011-47ad-99f8-f34a6b609a7c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Note, as needed HANA dataframes can also be saved back to SAP HANA tables, temporary tables, etc. or even as SQL views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf246c-9786-4595-bdf1-36264060b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdf_pcavectors.save('CC_PCAVECTORS', force=True ) # table_type='...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dbe5d-3f0d-4e95-ad5d-d571446ba4d5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, compose our final dataframe for the classification model, joining the selected feature columns with the PCA-Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143fa44-cded-40c3-96a6-7f44df2bc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final data for classification\n",
    "hdf_cc_classification=hdf_tmp.select('ComplaintID', 'Product', 'Issue', 'State', 'ZipCode', 'LABEL').set_index(\"ComplaintID\").join(hdf_pcavectors.set_index(\"ComplaintID\"))\n",
    "\n",
    "print(hdf_cc_classification.shape, '\\n')\n",
    "display(hdf_cc_classification.head(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fae7c-cc27-4790-9670-e64578dd5cf6",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Step 7: Build the AutoML classification model predicting monetary relief-response to consumers [12:300s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9d23d-fab7-450a-acd7-3a8508ce264d",
   "metadata": {},
   "source": [
    "Review the prepared HANA dataframe to train the classication model. Is it still the same as above ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e648fc-2e65-44c5-b328-45fa98ea73dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review the prepared HANA dataframe ...\n",
    "print(hdf_cc_classification.columns, '\\n')\n",
    "print(hdf_cc_classification.shape, '\\n')\n",
    "display(hdf_cc_classification.head(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c125fd-ca43-4000-8c69-41f0753fddb9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Prepare the feature columns as a Python list variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f2554-2b6d-4ff2-9561-1a7cf0f206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features for the classification model: what would be known at the point of time a complaint is issued?\n",
    "# Let's start to build a model solemnly using the PCA_Vector derived from complaint narratives' Text Embedding column\n",
    "class_features=['ComplaintNarrative_PCA_VECTOR'] \n",
    "\n",
    "# Variant of columns to poentially explore\n",
    "#class_features=['Product', 'State', 'ComplaintNarrative_PCA_VECTOR'] \n",
    "#class_features=['Product', 'State']                                  # ,'ComplaintNarrative_PCA_VECTOR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976c2ee-8684-43b9-a38b-47128f3a89b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Split the data to sample hold-out set of 20% of the data, not seen and used during model training, but used for model performance (predictive quality) evaluation (score).  \n",
    "Stratified sampling ensured same proportions of LABEL-values in both the training-sample and the hold-out sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cca95-968e-450a-bbed-66c7c10c8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a model training- and a houl-out sample for testing the classification model\n",
    "from hana_ml.algorithms.pal.partition import train_test_val_split\n",
    "train_ids, test_ids, _ = train_test_val_split(data=hdf_cc_classification.select('ComplaintID','LABEL'), partition_method='stratified', stratified_column='LABEL',\n",
    "                                                       training_percentage=0.8,\n",
    "                                                       testing_percentage=0.2,\n",
    "                                                       validation_percentage=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c6cc9-f8ec-43a0-b79a-8d7eb8e1eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the train_ids, test_ids from the sampling step before with the original data to compose the full classif_train- and classif_test-dataframes\n",
    "hdf_cc_classif_train=hdf_cc_classification.drop('LABEL').set_index(\"ComplaintID\").join(train_ids.set_index(\"ComplaintID\")) #.select('ComplaintID')\n",
    "display(hdf_cc_classif_train.shape)\n",
    "\n",
    "hdf_cc_classif_test=hdf_cc_classification.drop('LABEL').set_index(\"ComplaintID\").join(test_ids.set_index(\"ComplaintID\"))\n",
    "display(hdf_cc_classif_test.shape)\n",
    "\n",
    "display(hdf_cc_classif_train.head(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2352e-b71d-43d5-bded-411d446ad289",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "The Predictive Analysis Library (PAL) __AutoML-functions__ provides a means to __automatically explore different machine learning algorithms__ and algorithm __parameter combinations__ for a given machine learning scenario and data provided. It moreover evaluates __chaining of multiple algorithms__ into so-called algorithm pipelines. It thus benchmarks various algorithm configuration or pipelines against each other and seeks to improve to the best possible machine learning model using a genetic algorithm for this optimization approach.  \n",
    "<br>\n",
    "As a __target metric__ for the AutoML process to find the best model for, is that __we want the model to focus on best possible predictions__ for the __LABEL value: Closed_with_monetary_relief__, that is adressed with the __scorings-value \"F1_SCORE_Closed_with_monetary_relief'__. Apart from that, we are using a default configuration to start the AutoML process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01cba6-76bd-4b5f-8f45-374d62bad827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the PAL AutoML classification scenario and general settings \n",
    "from hana_ml.algorithms.pal.auto_ml import AutomaticClassification\n",
    "from hana_ml.visualizers.automl_progress import PipelineProgressStatusMonitor\n",
    "\n",
    "automl_class = AutomaticClassification(config_dict='default',      #'default' or 'light' AutoML classification configuration (selection of algorithms and paramters)  \n",
    "                                       population_size=10, generations=5,  offspring_size=10,\n",
    "                                       search_method='GA', successive_halving=True, fold_num=3,\n",
    "                                       random_seed=1234, max_layer=2, elite_number=5, \n",
    "                                       scorings={\"F1_SCORE_Closed_with_monetary_relief\": 1.0}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161bd30-9b9e-4a77-9e98-fd7501da32bc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The AutoML configuration determines which algorithms and parameter values get probed, it can certainly be adjusted as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e122ba-c409-4a26-92c0-c8a3a101b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classifier algorithm and their configurations included\n",
    "automl_class.display_config_dict(category=\"Classifier\")\n",
    "\n",
    "#automl_class.display_config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5172e6-36e5-4416-9d6c-442fcde8fc38",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Start the AutoML process and opening the AutoML Progress Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6a521-08b4-4de6-8d61-032f7ec72a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically, a database administrator would have constrained the database which can be consumed by a machine learning taks using workload classes \n",
    "automl_class.enable_workload_class(\"PAL_AUTOML_WORKLOAD\")\n",
    "#automl_class.disable_workload_class_check()\n",
    "progress_status_monitor = PipelineProgressStatusMonitor(connection_context=myconn, automatic_obj=automl_class)\n",
    "progress_status_monitor.start()\n",
    "\n",
    "automl_class.fit(data=hdf_cc_classif_train, key='ComplaintID', features=class_features, label='LABEL')\n",
    "\n",
    "print(automl_class.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f50799-454a-4107-aeb3-b542b5a64cb2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Explore the best models using the __Unified Model report__, overviewing the resulting best (elite) pipeline models found.\n",
    "- What is the __F1_SCORE_Closed_with_monetary_relief__ value achieved with the best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97d8e9-e39e-4231-9f36-3ca06123b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.visualizers.unified_report import UnifiedReport\n",
    "UnifiedReport(automl_class).build().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03051d5d-60d6-4ab8-89c0-2e545247d163",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Apply the __score()-method__ with the hold-out sample, to __evaluate the model performance (i.e. accuracy etc.)__ on data unseen during fitting of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde1e0d-5f26-43fc-be24-857d4c808da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pred_hdf, score_stats_hdf = automl_class.score( data=hdf_cc_classif_test, key='ComplaintID', features=class_features, label='LABEL')\n",
    "automl_class.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0685e0-a8a8-49da-8f15-3e37076f3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the statistics - what is the F1_SCORE_Closed_with_monetary_relief from the score-task compared to the fit-task\n",
    "score_stats_hdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f7702-0e7c-410e-ad43-6d16368cbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the predicted values in details for the complaints in the hold-out sample\n",
    "score_pred_hdf.filter('SCORE = \\'Closed_with_monetary_relief\\'').sort('CONFIDENCE', desc=True).select('ID', 'SCORE', 'CONFIDENCE').head(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a3750-c77e-4760-b5ab-f5f80deb51bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Use the __predict()-method__, if one is interested in the __predictions__ of company's response for new complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c1251-5776-4523-bcdd-b50e19226afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict company response-category for incoming complaints\n",
    "hdf_predictions = automl_class.predict( data=hdf_cc_classif_test.drop('LABEL'), key='ComplaintID', features=class_features) \n",
    "print(automl_class.runtime)\n",
    "\n",
    "hdf_predictions_monetary=hdf_predictions.filter('SCORES = \\'Closed_with_monetary_relief\\'').select('ID', 'SCORES') \n",
    "#hdf_predictions_monetary.head(5).collect() \n",
    "\n",
    "# Complaints predicted to be adressed by monetary relief\n",
    "hdf_result=consumercomplaints_hdf.select('ComplaintID', 'ConsumerComplaintNarrative').set_index(\"ComplaintID\").join(hdf_predictions_monetary.set_index(\"ID\"))\n",
    "hdf_result.rename_columns({'SCORES': 'PREDICTED_CompanyResponseToConsumer'}).head(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bb5c2-358f-465d-bb15-9375578f5361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00e1b8f-d55a-4965-ac12-fd81cd304e91",
   "metadata": {},
   "source": [
    "# Appendix - reference sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb2efb-5b6d-45a8-927f-265dc3af207f",
   "metadata": {},
   "source": [
    "## Loading the data sample (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798ac5d-97b0-4380-baa1-352c0c511556",
   "metadata": {},
   "source": [
    "Data and License  \n",
    "This repository contains a [CSV dataset example sample file](/datasets/complaint_clean.csv) that is a small dataset of service request tickets on complaints received about financial products and services. The dataset was obtained from [Kaggle Simulations](https://www.kaggle.com/sebastienverpile/consumercomplaintsdata/home?select=Consumer_Complaints.csv). It is originaly from [DATA.GOV](https://catalog.data.gov/dataset/consumer-complaint-database). The Dataset is licensed under [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/) that waives copyright interest in a work you've created and dedicates it to the world-wide public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac6738-6111-4ffc-881c-4e2b64d187dd",
   "metadata": {},
   "source": [
    "The consumer complaints data was downloaded as csv-file with the following filtering applied\n",
    "- field=all\n",
    "- has_narrative=true\n",
    "- company_received_max=2025-08-20\n",
    "- date_received_max=2025-06-30 and date_received_min=2011-12-01\n",
    "- company_public_response in\n",
    "    - Company believes it acted appropriately as authorized by contract or law\n",
    "    - Company disputes the facts presented in the complaint\n",
    "    - Company believes the complaint is the result of a misunderstanding\n",
    "    - Company believes complaint caused principally by actions of third party outside the control or direction of the company\n",
    "    - Company believes complaint is the result of an isolated error\n",
    "    - Company believes the complaint provided an opportunity to answer consumer%27s questions\n",
    "    - Company believes complaint represents an opportunity for improvement to better serve consumers\n",
    "    - Company can't verify or dispute the facts in the complaint \n",
    "- Company response to consumer in\n",
    "    - Closed with explanation \n",
    "    - Closed with non-monetary relief \n",
    "    - Closed with monetary relief \n",
    "    - Closed\n",
    "    \n",
    "Note, the company's optional public-facing response to a consumer's complaint. Companies can choose to select a response from a pre-set list of options that will be posted on the public database.\n",
    "\n",
    "See this direct download link of the filtered set [Filtered consumer-complaints download link](https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?company_public_response=Company%20believes%20it%20acted%20appropriately%20as%20authorized%20by%20contract%20or%20law&company_public_response=Company%20disputes%20the%20facts%20presented%20in%20the%20complaint&company_public_response=Company%20believes%20the%20complaint%20is%20the%20result%20of%20a%20misunderstanding&company_public_response=Company%20believes%20complaint%20caused%20principally%20by%20actions%20of%20third%20party%20outside%20the%20control%20or%20direction%20of%20the%20company&company_public_response=Company%20believes%20complaint%20is%20the%20result%20of%20an%20isolated%20error&company_public_response=Company%20believes%20the%20complaint%20provided%20an%20opportunity%20to%20answer%20consumer%27s%20questions&company_public_response=Company%20believes%20complaint%20represents%20an%20opportunity%20for%20improvement%20to%20better%20serve%20consumers&company_public_response=Company%20can%27t%20verify%20or%20dispute%20the%20facts%20in%20the%20complaint&company_received_max=2025-08-20&date_received_max=2025-06-30&date_received_min=2011-12-01&field=all&format=csv&has_narrative=true&no_aggs=true&size=128330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68b0d0-492d-40e2-94f2-0995aa8902b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9bc37-9008-40bc-b896-439c814bbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data into a pandas dataframe first\n",
    "consumer_data= pd.read_csv('./complaints-2025-08-21_03_58.csv')\n",
    "consumer_data.head(2)\n",
    "consumer_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3306e-4d05-441d-95a2-53d73d0e7b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consumer_data=consumer_data.rename(columns={\"Date received\": \"DateReceived\", \"Sub-product\": \"SubProduct\", \"Sub-issue\": \"SubIssue\", \n",
    "                                            \"Consumer complaint narrative\": \"ConsumerComplaintNarrative\", \"Company public response\": \"CompanyPublicResponse\", \n",
    "                                            \"Consumer consent provided?\": \"ConsumerConsentProvided\", \"Company response to consumer\": \"CompanyResponseToConsumer\", \n",
    "                                            \"Submitted via\": \"SubmittedVia\", \"Date sent to company\": \"DateSentToCompany\",  \"Timely response?\": \"TimelyResponse\",\n",
    "                                            \"Consumer disputed?\": \"ConsumerDisputed\", \"Complaint ID\": \"ComplaintID\", \"ZIP code\": \"ZipCode\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34072d-c4dd-447d-9028-c5aeb1712321",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_data.columns\n",
    "consumer_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27ca6b-c0fe-4413-b588-4ee157097659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move last column to first \n",
    "last_col = consumer_data.iloc[:, -1]  \n",
    "# extract last column \n",
    "consumer_data = pd.concat([last_col, consumer_data.iloc[:, :-1]], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fdd54-b7a2-4413-aa69-d7d02e2b0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_data.head(3)\n",
    "consumer_data.shape\n",
    "consumer_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1dffb-1bb9-4976-8a8a-22fb7927cb8e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Create HANA dataframe and HANA cloud database table from pandas dataframe, specifying specific HANA column datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576f53f-4f70-423b-ba23-92faaf698a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.dataframe import create_dataframe_from_pandas\n",
    "import pandas as pd\n",
    "consumercomplaints_hdf = create_dataframe_from_pandas(\n",
    "        myconn,\n",
    "        consumer_data, #.head(100000),\n",
    "        table_name=\"CONSUMER_COMPLAINTS_TMP\",\n",
    "        force=True,\n",
    "        replace=True,\n",
    "        drop_exist_tab=True\n",
    "        ,table_structure={\"ComplaintID\": \"INT\", \"DateReceived\" : \"NVARCHAR(10)\", \"Product\" : \"NVARCHAR(128)\", \"SubProduct\" : \"NVARCHAR(128)\", \n",
    "                          \"Issue\": \"NVARCHAR(128)\", \"SubIssue\"  : \"NVARCHAR(256)\", \"ConsumerComplaintNarrative\"  : \"NCLOB\", \"CompanyPublicResponse\": \"NCLOB\", \n",
    "                          \"Company\": \"NVARCHAR(128)\", \"State\": \"NVARCHAR(64)\", \"ZipCode\": \"NVARCHAR(8)\", \"Tags\": \"NVARCHAR(128)\", \"ConsumerConsentProvided\": \"NVARCHAR(24)\"\n",
    "                          ,\"SubmittedVia\": \"NVARCHAR(24)\", \"DateSentToCompany\"  : \"NVARCHAR(10)\", \"CompanyResponseToConsumer\"  : \"NVARCHAR(64)\", \n",
    "                          \"TimelyResponse\": \"NVARCHAR(8)\",  \"ConsumerDisputed\": \"NVARCHAR(8)\"} #, \"ComplaintNew\": \"NVARCHAR(5000)\",  \"ResponseNew\": \"NVARCHAR(5000)\"\n",
    "        )\n",
    "print(consumercomplaints_hdf.select_statement) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ad2be3f-0d29-4ebe-8a13-922eaaff82d5",
   "metadata": {},
   "source": [
    "# Note: this is SQL statements ########################################################\n",
    "DROP TABLE \"<schema>\".\"CONSUMER_COMPLAINTS\";\n",
    "CREATE COLUMN TABLE \"<schema>\".\"CONSUMER_COMPLAINTS\" (\"ComplaintID\" INTEGER,\n",
    "\"DateReceived\" DATE,\n",
    "\"Product\" NVARCHAR(128),\n",
    "\"SubProduct\" NVARCHAR(128),\n",
    "\"Issue\" NVARCHAR(128),\n",
    "\"SubIssue\" NVARCHAR(256),\n",
    "\"ConsumerComplaintNarrative\" NCLOB, /*NVARCHAR(4096),*/\n",
    "/*\"ConsumerComplaintNarrativeCHAR\" NVARCHAR(5000),*/\n",
    "\"ComplaintNarrative_EMBEDDING\" REAL_VECTOR(768),\n",
    "\"CompanyPublicResponse\" NVARCHAR(128),\n",
    "/*\"CompanyPublicResponseCHAR\" NVARCHAR(5000),*/\n",
    "\"Company\" NVARCHAR(128),\n",
    "\"State\" NVARCHAR(64),\n",
    "\"ZipCode\" NVARCHAR(8),\n",
    "\"Tags\" NVARCHAR(128),\n",
    "\"ConsumerConsentProvided\" NVARCHAR(24),\n",
    "\"SubmittedVia\" NVARCHAR(24),\n",
    "\"DateSentToCompany\" DATE,\n",
    "\"CompanyResponseToConsumer\" NVARCHAR(64),\n",
    "\"TimelyResponse\" NVARCHAR(8),\n",
    "\"ConsumerDisputed\" NVARCHAR(8)) UNLOAD PRIORITY 5 AUTO MERGE;\n",
    "\n",
    "SELECT count(*) FROM \"<schema>\".\"CONSUMER_COMPLAINTS\";\n",
    "\n",
    "Truncate TABLE \"<schema>\".\"CONSUMER_COMPLAINTS\";\n",
    "INSERT INTO \"<schema>\".\"CONSUMER_COMPLAINTS\" SELECT  \"ComplaintID\", TO_DATE(\"DateReceived\", 'MM/DD/YY') AS \"DateReceived\", \"Product\", \"SubProduct\", \"Issue\" ,\"SubIssue\" , \n",
    "       \"ConsumerComplaintNarrative\" , /*\"ComplaintNew\" as \"ConsumerComplaintNarrativeCHAR\",*/ NULL AS \"ComplaintNarrative_EMBEDDING\",\n",
    "       \"CompanyPublicResponse\", /* \"ResponseNew\" AS \"CompanyPublicResponseCHAR\",*/ \n",
    "       \"Company\", \"State\", \"ZipCode\", \"Tags\", \"ConsumerConsentProvided\" ,\"SubmittedVia\" ,\n",
    "       TO_DATE(\"DateSentToCompany\", 'MM/DD/YY') AS \"DateSentToCompany\", \"CompanyResponseToConsumer\", \"TimelyResponse\", \"ConsumerDisputed\"\n",
    "       FROM  \"<schema>\".\"CONSUMER_COMPLAINTS_TMP\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e23ec-5654-4e9f-8dfe-292b7c555511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
